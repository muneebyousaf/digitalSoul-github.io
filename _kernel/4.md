---
title: 4. DMA main principles
---
# Peripheral DMA

Peripheral DMA (Direct Memory Access) is a feature where certain device controllers have their own built-in DMA controller, which allows them to perform data transfers between peripheral devices and memory without CPU intervention. This capability provides several advantages:

1. **Reduced CPU Overhead**: With embedded DMA controllers, the CPU does not need to manage the data transfer. This reduces the burden on the CPU, allowing it to focus on more critical tasks.

2. **Faster Data Transfer**: DMA controllers are optimized for data transfer tasks and can move data more efficiently than the CPU. This results in faster data transfer rates.

3. **Efficient Data Streaming**: Some devices, like audio or video peripherals, require a continuous and uninterrupted data stream. An embedded DMA controller can ensure a consistent and smooth data flow, reducing the risk of interruptions or glitches.

4. **Improved System Responsiveness**: By offloading data transfer tasks to the DMA controller, the CPU is free to respond quickly to other system events and tasks, enhancing overall system responsiveness.

5. **Parallel Processing**: In systems with multiple peripheral devices that support embedded DMA controllers, concurrent data transfers can occur simultaneously, allowing for parallel processing of data.

Here's how the process typically works with devices that have embedded DMA controllers:

1. **Initialization**: During system boot or device initialization, the embedded DMA controller is configured. This includes setting up parameters such as source and destination addresses, data transfer size, and control signals.

2. **Request for Data Transfer**: When the peripheral device needs to send or receive data, it generates a request to the embedded DMA controller.

3. **DMA Transfer**: The embedded DMA controller takes control of the system bus and performs the data transfer directly between the peripheral device and memory. The CPU is not involved in the actual data movement.

4. **Completion Notification**: Once the data transfer is complete, the embedded DMA controller may generate an interrupt or notification to inform the CPU or relevant software component.

5. **Data Processing**: The CPU or software component can then process the data as needed.

6. **Repeat**: This process can be repeated for subsequent data transfers.

Examples of devices that commonly utilize embedded DMA controllers include:

- **Audio Codecs**: Audio controllers often have their own DMA controllers to stream audio data to and from memory.

- **Graphics Processing Units (GPUs)**: Some GPUs have embedded DMA engines to transfer textures and buffers to and from system memory.

- **Network Interface Cards (NICs)**: NICs may use DMA controllers for efficient packet handling and data transfer.

- **Storage Controllers**: Storage controllers like SATA or NVMe SSD controllers often have embedded DMA engines for fast data movement between storage media and memory.

To utilize embedded DMA controllers in the Linux kernel, device drivers typically interact with the DMA APIs and frameworks provided by the kernel. These APIs allow drivers to configure and manage DMA transfers, ensuring proper synchronization and data integrity while taking advantage of the embedded DMA capabilities of the device controller.
# DMA controllers

Some device controllers rely on an external DMA (Direct Memory Access) controller, which is a separate hardware component often integrated into the System-on-Chip (SoC). These external DMA controllers are responsible for managing data transfers between peripheral devices and system memory. To work with such external DMA controllers, device drivers need to submit DMA descriptors to the controller, providing instructions on how data transfers should be performed. Here's how this process typically works:

1. **Initialization**: During system initialization, the external DMA controller is configured. This involves setting up parameters such as the source and destination addresses, data transfer size, and control signals. The driver may also allocate memory for DMA descriptors.

2. **Descriptor Creation**: When the peripheral device needs to perform a data transfer, the device driver creates one or more DMA descriptors. These descriptors contain information about the data transfer, including the source and destination addresses, the number of bytes to transfer, and any relevant control settings.

3. **Descriptor Submission**: The driver submits the DMA descriptors to the external DMA controller. This submission typically involves writing the descriptor's information to specific registers or memory-mapped locations associated with the DMA controller.

4. **Data Transfer**: The external DMA controller takes control of the data transfer process. It reads the submitted descriptors and initiates the data transfer between the peripheral device and system memory.

5. **Completion Notification**: Once the data transfer is complete, the external DMA controller may generate an interrupt or notification to inform the driver or CPU of the transfer's status.

6. **Data Processing**: The CPU or relevant software component can then process the transferred data as needed.

7. **Descriptor Recycling**: In some cases, DMA descriptors can be reused for subsequent transfers by updating their contents with new transfer parameters.

Examples of devices that commonly rely on external DMA controllers include:

- **Networking Devices**: Network Interface Cards (NICs) often use external DMA controllers for efficient packet reception and transmission.

- **Storage Controllers**: Storage controllers, such as those used in SATA or NVMe SSDs, may rely on external DMA controllers for data transfers between storage media and memory.

- **High-Performance Peripherals**: Devices requiring high data throughput, such as high-resolution cameras or digital signal processors (DSPs), may use external DMA controllers.

- **External Interfaces**: External interfaces like USB and PCIe may utilize external DMA controllers to manage data transfers.

In the Linux kernel, drivers for devices using external DMA controllers interact with the DMA APIs and frameworks provided by the kernel. These APIs enable the creation, configuration, and management of DMA descriptors and provide synchronization mechanisms to ensure data integrity and consistency during transfers. The use of DMA descriptors allows drivers to instruct the external DMA controller on how to perform data transfers efficiently, taking into account the specific requirements of the peripheral device.

## Example 

Submitting DMA descriptors in a device driver typically involves interacting with DMA APIs provided by the operating system, such as the Linux kernel. Below is an example of how you might submit a DMA descriptor in a Linux kernel device driver using the DMA Engine framework. Please note that this is a simplified example for demonstration purposes.

Assume you have a device driver for a hypothetical peripheral device that needs to perform a DMA transfer from a buffer to a specific memory location.

```c
#include <linux/init.h>
#include <linux/module.h>
#include <linux/dmaengine.h>

#define DMA_TX_CHAN_NAME "dma_tx_channel"  // Name of the DMA channel

static struct dma_chan *dma_tx_channel;

static int my_device_probe(struct platform_device *pdev)
{
    struct dma_async_tx_descriptor *desc;
    struct scatterlist sg;
    dma_addr_t src_addr, dst_addr;
    int ret;

    // Request and configure the DMA channel
    dma_tx_channel = dma_request_slave_channel(&pdev->dev, DMA_TX_CHAN_NAME);
    if (!dma_tx_channel) {
        dev_err(&pdev->dev, "Failed to request DMA channel\n");
        return -ENOMEM;
    }

    // Initialize a scatter-gather list (SG entry) for the data
    sg_init_one(&sg, src_buffer, buffer_size);

    // Map the source and destination addresses
    src_addr = dma_map_single(&pdev->dev, src_buffer, buffer_size, DMA_TO_DEVICE);
    dst_addr = dma_map_single(&pdev->dev, dst_buffer, buffer_size, DMA_FROM_DEVICE);

    // Create a DMA descriptor for the transfer
    desc = dmaengine_prep_slave_sg(dma_tx_channel, &sg, 1, DMA_MEM_TO_MEM, DMA_PREP_INTERRUPT);
    if (!desc) {
        dma_unmap_single(&pdev->dev, src_addr, buffer_size, DMA_TO_DEVICE);
        dma_unmap_single(&pdev->dev, dst_addr, buffer_size, DMA_FROM_DEVICE);
        dma_release_channel(dma_tx_channel);
        return -ENOMEM;
    }

    // Configure the source and destination addresses
    dmaengine_desc_set_src(desc, src_addr);
    dmaengine_desc_set_dst(desc, dst_addr);

    // Submit the DMA transfer for execution
    ret = dmaengine_submit(desc);
    if (ret < 0) {
        dev_err(&pdev->dev, "Failed to submit DMA descriptor: %d\n", ret);
        dmaengine_terminate_async(dma_tx_channel);
        dma_unmap_single(&pdev->dev, src_addr, buffer_size, DMA_TO_DEVICE);
        dma_unmap_single(&pdev->dev, dst_addr, buffer_size, DMA_FROM_DEVICE);
        dma_release_channel(dma_tx_channel);
        return ret;
    }

    // Wait for the DMA transfer to complete
    dma_async_issue_pending(dma_tx_channel);
    ret = dma_sync_wait(dma_tx_channel, DMA_PREP_INTERRUPT);

    // Cleanup and release resources
    dma_unmap_single(&pdev->dev, src_addr, buffer_size, DMA_TO_DEVICE);
    dma_unmap_single(&pdev->dev, dst_addr, buffer_size, DMA_FROM_DEVICE);
    dmaengine_terminate_async(dma_tx_channel);
    dma_release_channel(dma_tx_channel);

    if (ret < 0) {
        dev_err(&pdev->dev, "DMA transfer error: %d\n", ret);
        return ret;
    }

    return 0;
}

static int my_device_remove(struct platform_device *pdev)
{
    // Clean up and release resources as needed
    return 0;
}

static struct platform_driver my_device_driver = {
    .driver = {
        .name = "my_device",
        .owner = THIS_MODULE,
    },
    .probe = my_device_probe,
    .remove = my_device_remove,
};

module_platform_driver(my_device_driver);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("Example DMA Driver");
```

In this example:

- We request a DMA channel named `DMA_TX_CHAN_NAME` using `dma_request_slave_channel`.

- We initialize a scatter-gather list (`sg`) for the data we want to transfer.

- We map the source and destination addresses using `dma_map_single`.

- We create a DMA descriptor for the transfer using `dmaengine_prep_slave_sg`. We specify the source and destination addresses and the transfer direction (in this case, `DMA_MEM_TO_MEM`).

- We submit the DMA transfer for execution using `dmaengine_submit`.

- We wait for the DMA transfer to complete using `dma_sync_wait`.

- Finally, we clean up and release resources.

Please note that this is a simplified example, and real-world scenarios may involve more complex configurations and error handling. The exact API functions and procedures may vary depending on the DMA controller and kernel version.


# DMA descriptors

DMA (Direct Memory Access) descriptors are data structures used in DMA transfers to describe the details of a data transfer between memory locations. These descriptors provide information to the DMA controller about the source and destination addresses, transfer size, transfer direction, and other control information. DMA descriptors help offload data transfers from the CPU, improving overall system performance.

A typical DMA descriptor contains the following fields:

1. Source Address: The starting address in memory from where data will be read for the transfer.

2. Destination Address: The starting address in memory where data will be written during the transfer.

3. Transfer Size: The number of bytes to be transferred.

4. Transfer Direction: Specifies whether the transfer is from source to destination (e.g., memory to peripheral) or vice versa (e.g., peripheral to memory).

5. Control Flags: Additional control bits or flags that may specify features such as burst mode, interrupt generation, and descriptor chaining.

6. Next Descriptor Pointer: In some systems, multiple descriptors can be linked together to form a chain. This field points to the next descriptor in the chain, allowing for sequential data transfers.

DMA descriptors enable efficient handling of complex data transfer scenarios, including scatter-gather (SG) transfers where data is scattered across multiple non-contiguous memory regions. These descriptors are often used in conjunction with DMA engines or controllers, which manage the actual data transfer based on the information provided in the descriptors.

Here's a simplified example of a DMA descriptor structure in C:

```c
struct dma_descriptor {
    dma_addr_t src_addr;       // Source address
    dma_addr_t dst_addr;       // Destination address
    size_t transfer_size;      // Transfer size in bytes
    unsigned int control_flags; // Control flags
    struct dma_descriptor *next; // Pointer to the next descriptor in the chain
};
```

In practice, the structure and fields of DMA descriptors can vary depending on the specific DMA controller and hardware platform being used. Device drivers and DMA frameworks provided by the operating system handle the creation and management of these descriptors to perform efficient data transfers.


# Cache constraints

Cache constraints refer to the limitations and requirements imposed by CPU caches on software, particularly in the context of optimizing memory access and ensuring data consistency. Understanding cache constraints is essential for writing high-performance and cache-friendly code in systems programming and kernel development.

Here are some common cache constraints and considerations:

1. Cache Coherency: Modern processors have multiple levels of caches (L1, L2, L3) that store copies of data from main memory. Ensuring cache coherency is critical to avoid data inconsistencies between CPU caches and main memory. This is typically handled by the hardware, but software developers must be aware of cache coherence issues, especially in multi-core systems.

2. Cache Line Size: CPUs access memory in fixed-size blocks called cache lines. Cache line sizes vary between processors but are typically 64 bytes. Accessing data that spans multiple cache lines can lead to inefficient memory access patterns, known as cache line bouncing.

3. Data Alignment: Accessing data at addresses not aligned to their natural size (e.g., 32-bit integers should be aligned on 4-byte boundaries) can lead to cache-related performance penalties. Proper data alignment helps maximize cache efficiency.

4. Cache Flush and Invalidation: In certain scenarios, such as when working with memory-mapped hardware registers or DMA transfers, it may be necessary to flush or invalidate cache lines to ensure data consistency. This operation can be expensive and should be used judiciously.

5. Cache-Friendly Data Structures: Designing data structures that minimize cache misses can significantly improve performance. For example, using contiguous memory for arrays, arranging data to minimize padding, and accessing data in a cache-friendly order can all help.

6. False Sharing: False sharing occurs when multiple threads or cores access data in the same cache line concurrently. This can lead to unnecessary cache invalidations and contention. Padding data structures to avoid false sharing is a common technique.

7. Cache-Aware Algorithms: Some algorithms are designed to take advantage of cache behavior. For example, cache-conscious sorting algorithms like merge sort or radix sort are designed to minimize cache misses during data access.

8. Cache Profiling and Optimization: Profiling tools can help identify cache-related performance issues in code. Techniques like loop unrolling, software prefetching, and cache blocking can be used for optimization.

9. Memory Barriers: Memory barrier instructions (e.g., memory fences) ensure that memory operations are completed in a specific order and can be crucial for maintaining cache coherency in multi-threaded applications.

Understanding and managing cache constraints is a complex and platform-dependent task. It often requires profiling, benchmarking, and careful consideration of the specific hardware and software environment. In kernel development, cache constraints are particularly important for optimizing critical sections of code, drivers, and other low-level components.

# DMA addressing constraints



1. **Physical Addresses (phys_addr_t):** These are the actual, physical locations in the computer's memory hardware. Every byte of RAM in a computer has a unique physical address. Devices that interact directly with memory, like DMA controllers, use physical addresses for memory access.

2. **Virtual Addresses (void *):** CPUs usually access memory through virtual addresses. Virtual memory allows the operating system to provide each process with the illusion of having its own private memory, even though it may be physically stored in different locations. The Memory Management Unit (MMU) in the CPU handles the translation from virtual to physical addresses. Programs interact with virtual addresses, and the MMU maps those virtual addresses to physical addresses as needed.

3. **DMA Controllers:** Direct Memory Access (DMA) controllers are specialized hardware units that can transfer data between memory and I/O devices without CPU intervention. These controllers operate at the physical address level, which means they interact directly with the physical memory hardware.

4. **IOMMU (Input/Output Memory Management Unit):** Some systems have an IOMMU, which acts as a memory address translator for devices that use DMA. It allows devices to use virtual addresses rather than physical addresses. This provides benefits like memory protection and better isolation between devices. However, to use an IOMMU, specific mappings between virtual and physical addresses must be established.

In summary, the choice between physical and virtual addresses depends on the context of the operation:

- Devices like DMA controllers typically operate with physical addresses since they do not have access to the CPU's MMU.
- CPUs interact with memory using virtual addresses, and the MMU handles the translation to physical addresses.
- IOMMUs can be used to allow devices to work with virtual addresses, but this requires setting up appropriate mappings.

Understanding the distinction between these address types is crucial for low-level system programming, especially in device driver development and ensuring that data is correctly transferred between devices and memory.




# DMA memory allocation constraints

DMA allows peripherals or devices to transfer data to and from memory without involving the CPU. To ensure DMA works correctly, there are specific memory requirements:

1. **Physically Contiguous Memory:** DMA controllers typically require memory regions that are physically contiguous. This means that the memory addresses of the allocated buffer are consecutive without any gaps. This requirement is imposed by the way DMA engines work, as they fetch data in a linear manner from memory.

2. **Memory Sources Suitable for DMA:** Not all memory sources are suitable for DMA. The following types of memory allocation are generally considered safe for DMA:

   - **Memory Allocated by kmalloc():** This function allocates small chunks of memory, often used for kernel data structures. It's suitable for DMA as long as the allocated buffer size is within the specified limit (typically up to 128 KB).

   - **Memory Allocated by __get_free_pages():** This function allocates larger chunks of physically contiguous memory, making it suitable for DMA as long as the buffer size is within the specified limit (typically up to 8 MB).

   - **Block I/O and Networking Buffers:** These buffers are explicitly designed to support DMA operations and are typically used for I/O operations where data transfer to/from devices is required. They ensure that data is physically contiguous and can be safely accessed by DMA.

However, there are some memory sources that are not suitable for DMA:

   - **Memory Allocated by vmalloc():** The `vmalloc()` function is used for allocating larger memory regions that are not necessarily physically contiguous. It is typically used for virtual memory and may not meet the DMA requirements.

   - **User Memory Allocated by malloc():** User space memory allocated by `malloc()` is not guaranteed to be physically contiguous and is generally not suitable for DMA operations.

Example:

```c
#include <linux/slab.h>
#include <linux/dma-mapping.h>

void perform_dma_operation(void) {
    size_t buffer_size = 4096; // Size of buffer in bytes
    void *dma_buffer;

    // Allocate physically contiguous memory using kmalloc
    dma_buffer = kmalloc(buffer_size, GFP_KERNEL);
    if (!dma_buffer) {
        printk(KERN_ERR "Failed to allocate DMA buffer\n");
        return;
    }

    // Perform DMA operations using dma_buffer
    // ...

    // Free the DMA buffer when done
    kfree(dma_buffer);
}
```

In this example, we allocate a DMA buffer using `kmalloc()`, which ensures that the memory is physically contiguous and suitable for DMA operations.

Keep in mind that DMA buffer allocation and management may vary depending on the specific hardware and drivers being used. It's essential to consult the documentation and guidelines provided by the hardware manufacturer and the Linux kernel DMA API when working with DMA in your driver code.



When it comes to DMA and userspace interaction, the kernel often plays a critical role in managing the memory buffers that are accessible by both kernel space and user space. This is particularly important for ensuring that DMA transfers between devices and user programs are safe and efficient. To achieve this, a dedicated user API may be used, often provided by the kernel.

Here's how this process typically works:

1. **Kernel Allocates DMA-Safe Memory:** The kernel allocates memory buffers that are suitable for DMA transfers. These buffers are physically contiguous and meet the requirements of DMA controllers. As mentioned earlier, the kernel can use functions like `kmalloc()` or `vmalloc()` to allocate DMA-safe memory.

2. **Mapping Kernel Memory to User Space:** To make these buffers accessible from user space, the kernel provides an API for user programs to map these kernel-allocated buffers into their own address space. The most common way to achieve this is through the `mmap()` system call.

3. **User Space Access:** Once the memory is mapped into the user space address space using `mmap()`, user programs can access these memory regions as if they were regular user space memory. This allows user programs to read from or write to the buffers directly.

4. **Kernel-User Space Coordination:** To ensure data integrity and synchronization between kernel and user space, coordination mechanisms such as locking, synchronization primitives, and memory barriers may be used. For example, a user program may use synchronization primitives like mutexes or semaphores to coordinate access to the shared DMA buffer.

5. **Buffer Management and Cleanup:** The kernel is responsible for managing the lifecycle of the DMA buffers, including their allocation and deallocation. When the buffers are no longer needed, the kernel ensures they are properly released to avoid memory leaks.

Example (simplified):

```c
// Kernel module code to allocate DMA-safe memory and provide mmap support
#include <linux/module.h>
#include <linux/fs.h>
#include <linux/miscdevice.h>
#include <linux/dma-mapping.h>

static void *dma_buffer;
static size_t buffer_size = 4096; // Size of the DMA buffer

static int dma_mmap(struct file *filp, struct vm_area_struct *vma) {
    unsigned long pfn;
    unsigned long size = vma->vm_end - vma->vm_start;

    if (size > buffer_size)
        return -EINVAL;

    // Map the DMA buffer into user space
    pfn = virt_to_phys(dma_buffer) >> PAGE_SHIFT;
    if (remap_pfn_range(vma, vma->vm_start, pfn, size, vma->vm_page_prot)) {
        printk(KERN_ERR "Failed to map DMA buffer to user space\n");
        return -EAGAIN;
    }

    return 0;
}

static const struct file_operations dma_fops = {
    .owner = THIS_MODULE,
    .mmap = dma_mmap,
};

static struct miscdevice dma_device = {
    .minor = MISC_DYNAMIC_MINOR,
    .name = "dma_example",
    .fops = &dma_fops,
};

static int __init dma_init(void) {
    dma_buffer = dma_alloc_coherent(NULL, buffer_size, &dma_handle, GFP_KERNEL);
    if (!dma_buffer) {
        printk(KERN_ERR "Failed to allocate DMA buffer\n");
        return -ENOMEM;
    }

    return misc_register(&dma_device);
}

static void __exit dma_exit(void) {
    misc_deregister(&dma_device);
    dma_free_coherent(NULL, buffer_size, dma_buffer, dma_handle);
}





module_init(dma_init);
module_exit(dma_exit);
MODULE_LICENSE("GPL");
```

In this simplified example, a kernel module allocates a DMA-safe buffer and provides mmap support to allow user programs to map and access the buffer in user space. Userspace programs can then use `mmap()` to map the DMA buffer into their address space and perform read and write operations on it safely. The kernel module also ensures proper buffer allocation and cleanup.




User space can access the DMA buffer mapped by the kernel in the following way:

1. **Using `mmap()`**: The primary method for user space programs to access the DMA buffer is through the `mmap()` system call. Here's how a user space program can map the DMA buffer:

    ```c
    #include <stdio.h>
    #include <fcntl.h>
    #include <sys/mman.h>

    int main() {
        int fd = open("/dev/dma_example", O_RDWR); // Open the device file
        if (fd < 0) {
            perror("Failed to open device file");
            return 1;
        }

        size_t buffer_size = 4096; // Size of the DMA buffer
        void *mapped_buffer = mmap(NULL, buffer_size, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);

        if (mapped_buffer == MAP_FAILED) {
            perror("Failed to map DMA buffer");
            close(fd);
            return 1;
        }

        // Now, you can read from or write to 'mapped_buffer' as needed

        munmap(mapped_buffer, buffer_size); // Unmap the buffer when done
        close(fd); // Close the device file

        return 0;
    }
    ```

    In this example, the user space program opens the device file created by the kernel module (e.g., `/dev/dma_example`) and uses `mmap()` to map the DMA buffer into its own address space. After mapping, the user space program can access the buffer directly as if it were regular memory.

2. **Reading/Writing**: Once the DMA buffer is mapped, the user space program can read from or write to the buffer like any other memory region. For example:

    ```c
    unsigned char *dma_data = (unsigned char *)mapped_buffer;
    
    // Write data to the DMA buffer
    dma_data[0] = 0x55;
    
    // Read data from the DMA buffer
    unsigned char value = dma_data[0];
    ```

3. **Unmapping**: When the user space program is done with the DMA buffer, it should unmap the buffer using `munmap()`. This releases the mapped memory region.

4. **Closing the Device File**: It's important to close the device file using `close()` when the program is finished using it to release system resources properly.

By following these steps, user space programs can safely and efficiently access DMA buffers shared with the kernel.
