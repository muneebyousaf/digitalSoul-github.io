---
title: 3. Concurrent Access to Resources- Locking
---
# Using spinlocks 2/3
The `spin_lock_irqsave` and `spin_unlock_irqrestore` functions are used in the Linux kernel to manage spin locks while also temporarily disabling and restoring interrupts on the local CPU. These functions are particularly useful when dealing with spin locks that can be accessed in both process context and interrupt context to prevent deadlocks and ensure proper synchronization.

Here's how these functions work:

1. **spin_lock_irqsave**:
   - `spin_lock_irqsave(spinlock_t *lock, unsigned long flags)` is used to acquire a spin lock while also saving the current interrupt state.
   - `lock` is a pointer to the spin lock you want to acquire.
   - `flags` is a variable that will be used to save the current interrupt state on the local CPU.

   When you call `spin_lock_irqsave`, it does the following:
   - Disables local interrupts on the CPU where it's called from.
   - Acquires the spin lock `lock`.
   - Saves the current interrupt state in the `flags` variable.

   This function ensures that interrupts are disabled while acquiring the spin lock to prevent interrupt handlers from trying to acquire the same lock and causing deadlocks.

2. **spin_unlock_irqrestore**:
   - `spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)` is used to release a spin lock and restore the interrupt state to what it was when `spin_lock_irqsave` was called.
   - `lock` is a pointer to the spin lock you want to release.
   - `flags` is the variable containing the saved interrupt state.

   When you call `spin_unlock_irqrestore`, it does the following:
   - Releases the spin lock `lock`.
   - Restores the interrupt state to what it was when `spin_lock_irqsave` was called using the `flags` variable.

   This function ensures that interrupts are restored to their original state after releasing the spin lock.

Example usage:

```c
spinlock_t my_spinlock;
unsigned long irq_flags;

// Initialize the spin lock
spin_lock_init(&my_spinlock);

// Acquire the spin lock while saving the interrupt state
spin_lock_irqsave(&my_spinlock, irq_flags);

// Critical section protected by the spin lock
// ...

// Release the spin lock and restore the interrupt state
spin_unlock_irqrestore(&my_spinlock, irq_flags);

// Continue with normal execution
```

In this example, `spin_lock_irqsave` is used to acquire the spin lock `my_spinlock` while saving the current interrupt state in `irq_flags`. After the critical section is completed, `spin_unlock_irqrestore` is used to release the spin lock and restore the interrupt state to its original value. This ensures that interrupts are correctly managed in both process and interrupt contexts, preventing potential deadlocks.

# Using spinlocks 3/3

The `spin_lock_bh` and `spin_unlock_bh` functions in the Linux kernel are used to acquire and release spin locks while disabling and enabling software interrupts, specifically the software interrupt context known as the "bottom half." These functions are designed for situations where you need to protect shared data accessed in process context and in a soft interrupt (bottom half). Unlike `spin_lock_irqsave` and `spin_unlock_irqrestore`, these functions do not disable hardware interrupts.

Here's how these functions work:

1. **spin_lock_bh**:
   - `spin_lock_bh(spinlock_t *lock)` is used to acquire a spin lock while disabling software interrupts (bottom half).
   - `lock` is a pointer to the spin lock you want to acquire.

   When you call `spin_lock_bh`, it does the following:
   - Disables only software interrupts (bottom half) on the CPU where it's called from.
   - Acquires the spin lock `lock`.

   This function is suitable for protecting data accessed in process context and within a soft interrupt handler. It's a more lightweight locking mechanism than disabling all interrupts, making it efficient when you only need to prevent concurrent access from software interrupts.

2. **spin_unlock_bh**:
   - `spin_unlock_bh(spinlock_t *lock)` is used to release a spin lock while enabling software interrupts (bottom half).
   - `lock` is a pointer to the spin lock you want to release.

   When you call `spin_unlock_bh`, it does the following:
   - Releases the spin lock `lock`.
   - Enables software interrupts (bottom half) on the CPU.

   This function ensures that software interrupts are reenabled after releasing the spin lock, allowing soft interrupt handlers to execute when needed.

Example usage:

```c
spinlock_t my_spinlock;

// Initialize the spin lock
spin_lock_init(&my_spinlock);

// Acquire the spin lock while disabling software interrupts (bottom half)
spin_lock_bh(&my_spinlock);

// Critical section protected by the spin lock
// ...

// Release the spin lock and enable software interrupts (bottom half)
spin_unlock_bh(&my_spinlock);

// Continue with normal execution, allowing soft interrupts to run
```

In this example, `spin_lock_bh` is used to acquire the spin lock `my_spinlock` while disabling software interrupts (bottom half). After the critical section is completed, `spin_unlock_bh` is used to release the spin lock and enable software interrupts, allowing soft interrupt handlers to execute as needed.


# `CONFIG_PROVE_LOCKING` and `CONFIG_DEBUG_ATOMIC_SLEEP`

`CONFIG_PROVE_LOCKING` and `CONFIG_DEBUG_ATOMIC_SLEEP` are kernel configuration options that are useful for debugging and verifying the correctness of locking mechanisms and atomic operations in the Linux kernel. They are typically used in development and debugging scenarios to catch potential issues related to concurrency and synchronization.

Here's an explanation of each option with examples:

1. `CONFIG_PROVE_LOCKING`:
   - This option enables runtime verification of locking correctness within the kernel.
   - It checks that locks are acquired and released in a proper order to avoid deadlocks and other synchronization issues.
   - It can detect problems such as lock inversion and circular dependencies in lock acquisition.
   - When enabled, it adds runtime checks for locking correctness and generates warning messages if any issues are detected.

   Example:
   ```shell
   $ make menuconfig
   # Enable "CONFIG_PROVE_LOCKING" in the kernel configuration menu
   ```

   Usage:
   Once enabled, run your kernel with this configuration. If there are any locking-related issues, the kernel will generate warning messages indicating the problem. You can then use these messages to identify and fix the issues in your code.

2. `CONFIG_DEBUG_ATOMIC_SLEEP`:
   - This option enables debugging checks for atomic sleep within the kernel.
   - It checks that certain sleep primitives are not used in atomic contexts, which can lead to kernel crashes or other unexpected behavior.
   - It helps catch scenarios where code intended to sleep (block) is executed within an atomic context, which is not allowed.

   Example:
   ```shell
   $ make menuconfig
   # Enable "CONFIG_DEBUG_ATOMIC_SLEEP" in the kernel configuration menu
   ```

   Usage:
   Once enabled, if any part of the kernel tries to sleep within an atomic context (where sleep is not allowed), the kernel will generate warning messages or trigger debugging checks. This helps identify and fix potential issues related to atomic sleep violations.

It's important to note that these options are primarily used by kernel developers and maintainers during the development and testing phases of kernel code. They are not typically enabled in production kernels, as they add runtime overhead and are intended for debugging purposes.

When using these options, developers should pay attention to the warning messages and debugging output generated by the kernel. These messages provide valuable insights into potential concurrency and synchronization issues within the kernel codebase, allowing developers to improve code quality and reliability.

# Concurrency issues 

The Kernel Concurrency SANitizer (KCSAN) is a dynamic race detector framework introduced in the Linux kernel starting from version 5.8. It is designed to help identify and detect concurrency issues, primarily data races, in the Linux kernel codebase. Data races occur when multiple threads or execution contexts access shared data without proper synchronization, leading to unpredictable behavior and potential system instability.

Here are some key points about the KCSAN framework:

1. **Purpose**: KCSAN is used to find and diagnose concurrency issues within the Linux kernel. It helps developers identify potential data races early in the development and testing phases.

2. **Dynamic Race Detector**: KCSAN operates as a dynamic race detector, meaning it detects race conditions and data races during runtime when the kernel is executing. It does this by instrumenting the kernel code to track memory accesses and detect concurrent access patterns.

3. **Compile-Time Instrumentation**: KCSAN relies on compile-time instrumentation to insert the necessary code into the kernel source during compilation. This instrumentation allows KCSAN to track memory accesses and identify potential race conditions.

4. **Concurrency Issues**: KCSAN mainly targets the detection of data races, which occur when multiple threads or execution contexts access shared data without proper synchronization. These races can lead to incorrect results, crashes, or other unexpected behavior.

5. **Kernel Configuration**: To use KCSAN, you need to enable the `CONFIG_KCSAN` kernel configuration option. This option activates KCSAN instrumentation and runtime checks.

6. **Usage**: Once `CONFIG_KCSAN` is enabled, you can run the kernel with KCSAN support. During execution, KCSAN will monitor memory accesses and report any potential data races it detects.

7. **Diagnosis and Debugging**: KCSAN reports data race conditions as they occur, providing information about the involved code and memory locations. Developers can use this information to diagnose and fix the concurrency issues in the kernel.

It's important to note that KCSAN is a valuable tool for kernel developers and maintainers to improve the reliability and correctness of the Linux kernel. However, it does add some runtime overhead due to the instrumentation and checking, so it is typically used during development and testing phases rather than in production kernels.

Developers interested in using KCSAN should refer to the official Linux kernel documentation and resources for detailed information on enabling and utilizing KCSAN for detecting concurrency issues in their kernel code.



#  Read-Copy-Update (RCU)

The Read-Copy-Update (RCU) API in the Linux kernel is a synchronization mechanism designed for efficient and scalable read-side access to shared data, even in the presence of concurrent updates. RCU is particularly useful in scenarios where multiple threads or CPUs need to read shared data frequently while occasional updates to the data structure occur.

Here are some key components and concepts of the RCU API:

1. **Read-Side Access**: In RCU, the read-side access to shared data is lockless and extremely fast. Readers do not acquire any locks or mutual exclusion mechanisms. This allows multiple readers to access data concurrently without blocking each other.

2. **Grace Periods**: RCU uses the concept of grace periods to manage updates. A grace period is a period during which no references to the old data structure are held by any readers. When a grace period is over, it's guaranteed that no readers are using the old data.

3. **Updating Data**: When a writer wants to update shared data, it typically creates a new version of the data structure. The old version continues to be accessible to readers during the grace period, and the new version is made available to readers once the grace period ends.

4. **API Functions**: The RCU API provides a set of functions that writers and readers can use to coordinate access to shared data. These functions include `rcu_read_lock()` and `rcu_read_unlock()` for readers and various `rcu_xxx` functions for writers, such as `rcu_assign_pointer()` and `synchronize_rcu()`.

5. **Memory Reclamation**: RCU ensures that memory used by old data structures is not reclaimed until all readers have completed their access to the old data. This is typically done by delaying the memory reclamation until the end of the grace period.

6. **RCU Variants**: The Linux kernel includes several variants of RCU, such as "classic RCU," "preemptible RCU," and "tree RCU." Each variant is optimized for different use cases and kernel configurations.

7. **Usage**: Developers use the RCU API to protect shared data structures from concurrent modification. Writers use RCU primitives to coordinate updates, while readers use RCU primitives to access data without locking.

Here's a basic example of how the RCU API might be used in the Linux kernel:

```c
#include <linux/rcupdate.h>

// Shared data structure
struct my_data {
    int value;
    // ... other fields
};

// Writer code to update shared data
void update_shared_data(struct my_data *new_data) {
    // Assign the new data structure
    rcu_assign_pointer(shared_data, new_data);
    synchronize_rcu(); // Wait for readers to finish using the old data
}

// Reader code to access shared data
int read_shared_data(void) {
    int result;
    rcu_read_lock();
    result = rcu_dereference(shared_data)->value;
    rcu_read_unlock();
    return result;
}
```

In this example, `rcu_assign_pointer()` is used by the writer to update the shared data, and `rcu_dereference()` is used by the reader to access it. The `synchronize_rcu()` call ensures that readers have completed their access to the old data before it is reclaimed.

RCU is a powerful synchronization mechanism used extensively in the Linux kernel to optimize read-side access to shared data structures. It helps improve scalability and performance in multi-core systems while maintaining correctness and data integrity.

# Atomic variables

In the context of the Linux kernel and concurrent programming, atomic variables are a fundamental concept used to ensure that certain operations on shared data structures are performed atomically, without being interrupted or interleaved by other threads or CPUs. Atomic operations are essential for managing concurrency and avoiding data corruption in a multi-threaded or multi-processor environment.

Here are some key points about atomic variables in the Linux kernel:

1. **Atomic Operations**: Atomic variables are used in conjunction with atomic operations, which are hardware-supported operations that can't be interrupted. Common atomic operations include atomic increments, decrements, swaps, and bit-wise operations.

2. **Atomic Types**: In the Linux kernel, atomic variables are typically represented using specific atomic types, such as `atomic_t`, `atomic64_t`, and others. These types are designed to be used in shared data structures and manipulated with atomic operations.

3. **Lock-Free Algorithms**: Atomic variables are often used to implement lock-free algorithms, which are algorithms that allow multiple threads to access shared data concurrently without traditional locks or mutexes. This can lead to improved performance in multi-core systems.

4. **Usage**: Atomic variables are used in situations where a shared variable needs to be modified or accessed by multiple threads or CPUs simultaneously. For example, they can be used to implement reference counting, counters, flags, and other synchronization primitives.

5. **Memory Barriers**: Atomic operations typically include memory barriers, which are instructions that ensure proper memory ordering. Memory barriers are essential for enforcing memory visibility and avoiding race conditions.

Here's a basic example of how an atomic variable might be used in the Linux kernel:

```c
#include <linux/atomic.h>

atomic_t my_counter = ATOMIC_INIT(0);

void increment_counter(void) {
    atomic_inc(&my_counter); // Atomically increment the counter
}

int get_counter_value(void) {
    return atomic_read(&my_counter); // Atomically read the counter value
}
```

In this example, `atomic_inc()` is used to increment `my_counter` atomically, and `atomic_read()` is used to read its value atomically. These operations ensure that multiple threads or CPUs can safely modify and access the counter without the need for additional locks or synchronization mechanisms.

Atomic variables are a critical building block for implementing concurrency control in the Linux kernel, and they play a crucial role in ensuring that concurrent operations on shared data remain safe and predictable.
