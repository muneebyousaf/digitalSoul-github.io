---
title: 5. Kernel APIs for DMA
---
# dma-mapping vs. dmaengine vs. dma-buf

DMA (Direct Memory Access) is a technique that allows peripherals and devices to access system memory (RAM) directly without involving the CPU. In the Linux kernel, several subsystems and frameworks exist to manage and facilitate DMA operations. Three key components related to DMA management in the Linux kernel are `dma-mapping`, `dmaengine`, and `dma-buf`.

1. **dma-mapping**:
   
   - **Purpose**: `dma-mapping` is a core framework in the Linux kernel responsible for managing DMA memory mappings. It provides a consistent and efficient way to map and unmap memory for DMA operations.
   
   - **Usage**: Device drivers can use `dma-mapping` to map physical memory (kernel memory) for DMA transfers. It handles issues related to cache coherency, alignment, and provides a standard API for mapping and unmapping memory.
   
   - **Typical Use Cases**: It is commonly used in device drivers for a wide range of peripherals and devices that require DMA access to system memory.

2. **dmaengine**:
   
   - **Purpose**: `dmaengine` is a subsystem in the Linux kernel designed to provide a generic API for configuring and managing DMA channels across various hardware controllers. It abstracts the hardware-specific details of different DMA controllers, making it easier for device drivers to perform DMA transfers.
   
   - **Usage**: Device drivers can use the `dmaengine` API to request and configure DMA channels, set up DMA descriptors, and initiate DMA transfers. It supports a wide range of DMA controllers, making it possible to write device drivers that work across different hardware platforms.

   - **Typical Use Cases**: It is used in device drivers for hardware components such as audio codecs, network interfaces, storage devices, and more that require efficient DMA operations.

3. **dma-buf**:
   
   - **Purpose**: `dma-buf` (DMA buffer) is a framework in the Linux kernel designed for sharing buffers between different devices or subsystems that support DMA. It provides a standardized API for exporting and importing DMA-contiguous memory buffers.
   
   - **Usage**: It allows for the creation of DMA-buf objects representing memory buffers that can be shared among different drivers, subsystems, or even between user space and kernel space. These buffers can be efficiently transferred between devices that support DMA-buf.

   - **Typical Use Cases**: It is used in scenarios where different subsystems or devices need to share memory buffers, such as graphics processing units (GPUs), video encoders/decoders, camera sensors, and other multimedia components.

, these three components play different roles in managing and facilitating DMA operations:

- **dma-mapping**: Provides a consistent way to map/unmap memory for DMA transfers.
- **dmaengine**: Offers a generic API for configuring and managing DMA channels across different hardware controllers.
- **dma-buf**: Enables the sharing of DMA-contiguous memory buffers between different devices or subsystems.

The choice of which component to use depends on the specific requirements of the device driver and the use case it addresses. Device drivers may use one or more of these components, depending on their needs.

# dma-mapping: Coherent or streaming DMA mappings

DMA (Direct Memory Access) memory mappings, two common approaches are used: coherent mappings and streaming mappings. These approaches determine how the kernel and device driver manage the memory buffers used for DMA transfers. Let's explore each of these approaches with examples:

1. **Coherent Mappings**:

   - **Purpose**: Coherent mappings are used when the memory buffer needs to be simultaneously accessible by both the CPU (Central Processing Unit) and the DMA-capable device. The key characteristic is that the memory is located in a cache-coherent memory area, ensuring that any changes made by the CPU are immediately visible to the device and vice versa.

   - **Usage Scenario**: Coherent mappings are typically allocated for the entire duration that the module (device driver) is loaded because they require specific memory regions that are cache-coherent. They are suitable for scenarios where data consistency between the CPU and device is critical.

   - **Example**:

     ```c
     // Allocate a coherent DMA buffer of size 4 KB
     dma_addr_t dma_handle;
     void *coherent_buffer = dma_alloc_coherent(dev, 4096, &dma_handle, GFP_KERNEL);

     // Access the coherent buffer from CPU and set data
     strcpy(coherent_buffer, "Hello, DMA!");

     // Perform DMA transfer using the coherent buffer
     dma_transfer_function(dma_handle);

     // The device can access the buffer immediately without cache issues
     ```

2. **Streaming Mappings**:

   - **Purpose**: Streaming mappings are used when the driver provides an already allocated buffer, and the kernel sets up the DMA mapping only for each individual transfer. These mappings are not cache-coherent, and the DMA registers on the hardware handle the data transfer.

   - **Usage Scenario**: Streaming mappings are useful when you want to minimize memory overhead and keep DMA registers available for other transfers. They are set up for each transfer and are less expensive to use on some platforms, especially when cache coherence is not required.

   - **Example**:

     ```c
     // Allocate a buffer for the DMA transfer (no mapping yet)
     void *dma_buffer = kmalloc(4096, GFP_KERNEL);

     // Prepare data in the buffer
     strcpy(dma_buffer, "Hello, DMA!");

     // Perform DMA transfer, setting up mapping for this transfer only
     dma_addr_t dma_handle;
     dma_map_single(dev, dma_buffer, 4096, DMA_TO_DEVICE);
     dma_transfer_function(dma_handle);

     // Clean up after the transfer
     dma_unmap_single(dev, dma_handle, 4096, DMA_TO_DEVICE);
     kfree(dma_buffer);
     ```

 coherent mappings are used when data consistency between the CPU and device is crucial, and memory buffers are allocated for the entire module's lifetime. Streaming mappings are used when memory buffers are provided by the driver and mapped only for specific DMA transfers, making them more efficient in terms of memory usage. The choice between these approaches depends on the specific requirements and constraints of the DMA operation and the target hardware.
 
#  dma-mapping: memory addressing constraints
 
 The DMA addressing capability of a system is crucial for determining the range of physical memory that DMA operations can access. Here are some key points regarding DMA addressing capability and related functions in the Linux kernel:

1. **Default DMA Addressing Capability**:
   - By default, DMA controllers are assumed to have a 32-bit addressing capability, allowing them to access the lower 4 GB of physical memory.
   - This assumption is suitable for many systems, but some platforms may have different addressing capabilities.

2. **Adjusting DMA Addressing Capability**:
   - Some platforms may require adjustments to the DMA addressing capability. For example:
     - Increasing the addressing capability: When a system has memory regions beyond the 4 GB boundary (e.g., highmem), DMA controllers may need an increased addressing capability to access this extended memory.
     - Decreasing the addressing capability: Older systems with ISA devices may have limitations on the maximum address range accessible by DMA controllers.
   - These adjustments are necessary to ensure that DMA mappings do not fail due to address limitations.

3. **DMA Mask**:
   - The DMA addressing capability for a specific device is stored in a device-specific mask.
   - The mask represents the highest physical address that the DMA controller can access.
   - It's essential to set the DMA mask consistently for a device before allocating DMA buffers.

4. **Setting DMA Mask and Coherence**:
   - The `dma_set_mask_and_coherent` function is used to set the DMA mask for a device and specify whether the memory should be coherent.
   - The `mask` parameter represents the DMA addressing capability for the device.
   - Coherent memory ensures cache coherence between the CPU and the DMA controller, which is crucial for consistent data access.

5. **Maximum and Optimal Buffer Sizes**:
   - The Linux kernel provides functions to query the maximum and optimal buffer sizes for DMA mappings.
   - `dma_max_mapping_size` returns the maximum buffer size that can be mapped for DMA.
   - `dma_opt_mapping_size` returns the optimal buffer size, which may be smaller than the maximum for efficiency reasons.

These functions help device drivers adjust the DMA addressing capability, allocate memory buffers that fit within the addressing limits, and ensure proper cache coherence for DMA operations. Properly managing DMA addressing capability is essential for reliable and efficient DMA transfers in various hardware configurations.
# dma-mapping: Allocating coherent memory mappings

The `dma_alloc_coherent` and `dma_free_coherent` functions are commonly used in Linux kernel device drivers to allocate and free coherent DMA memory buffers. These functions ensure that the allocated memory is physically contiguous and cache-coherent, making it suitable for DMA operations. Here's an explanation of each function:

1. **`dma_alloc_coherent` Function**:

   - `dma_alloc_coherent` is used to allocate a coherent DMA memory buffer.

   - **Parameters**:
     - `dev`: A pointer to the `struct device` associated with the device for which DMA memory is being allocated. This is used to determine the DMA addressing capability and ensure proper memory allocation.

     - `size`: The size of the DMA buffer to allocate in bytes. This specifies the amount of memory needed for DMA transfers.

     - `handle`: An output parameter that returns the DMA bus address associated with the allocated memory. This address is used by the DMA controller to access the memory.

     - `gfp`: Standard GFP (Get Free Pages) flags that control the memory allocation behavior. These flags specify how memory allocation should be handled, such as memory priority and whether the allocation can sleep.

   - **Return Value**:
     - The function returns a pointer to the CPU-accessible memory buffer that was allocated. This memory is both physically contiguous and cache-coherent.

   - **Usage**:
     - After calling `dma_alloc_coherent`, the driver can use the returned memory buffer for DMA operations. The `handle` parameter contains the DMA bus address, which should be provided to the DMA controller for DMA transfers.

2. **`dma_free_coherent` Function**:

   - `dma_free_coherent` is used to release a coherent DMA memory buffer that was previously allocated using `dma_alloc_coherent`.

   - **Parameters**:
     - `dev`: A pointer to the `struct device` associated with the device for which DMA memory was allocated. This ensures that the memory is released correctly.

     - `size`: The size of the DMA buffer that was allocated, specified in bytes.

     - `cpu_addr`: A pointer to the CPU-accessible memory buffer that was previously allocated using `dma_alloc_coherent`.

     - `handle`: The DMA bus address associated with the allocated memory.

   - **Usage**:
     - When DMA memory is no longer needed, the driver should call `dma_free_coherent` to release the memory resources. The function takes care of properly freeing the memory and cleaning up associated resources.

These functions simplify the allocation and management of coherent DMA memory, ensuring that the memory is suitable for DMA operations and properly synchronized with the CPU's cache. They are essential for device drivers that need to perform DMA transfers in the Linux kernel.

# dma-mapping: Setting up streaming memory mappings (single) 

The `dma_map_single` and `dma_unmap_single` functions in Linux are used for mapping and unmapping a single, physically contiguous buffer for DMA transfers. These functions ensure that the buffer is suitable for DMA operations and handle any necessary memory synchronization. Here's an explanation of each function along with examples:

1. **`dma_map_single` Function**:

   - `dma_map_single` is used to map a single buffer for DMA transfers.

   - **Parameters**:
     - `dev`: A pointer to the `struct device` associated with the device for which DMA is being performed. This is used to determine the DMA addressing capability and ensure proper memory mapping.

     - `addr`: A pointer to the CPU-accessible buffer that needs to be mapped for DMA. This buffer contains the data that will be transferred via DMA.

     - `size`: The size of the buffer in bytes that needs to be mapped for DMA transfers.

     - `dir`: An enum specifying the data direction for DMA transfers. It can be one of the following:
       - `DMA_BIDIRECTIONAL`: Data can be transferred in both directions (to and from the device).
       - `DMA_TO_DEVICE`: Data is transferred from the CPU to the device.
       - `DMA_FROM_DEVICE`: Data is transferred from the device to the CPU.

   - **Return Value**:
     - The function returns a `dma_addr_t` value, which represents the DMA bus address corresponding to the mapped buffer. This address should be provided to the DMA controller for use in DMA transfers.

   - **Usage Example**:
     ```c
     #include <linux/dma-mapping.h>

     struct device *my_device;  // Pointer to your device structure
     void *buffer;              // Pointer to your CPU-accessible buffer
     size_t buffer_size;        // Size of the buffer
     dma_addr_t dma_addr;

     // Assuming buffer and buffer_size are initialized
     dma_addr = dma_map_single(my_device, buffer, buffer_size, DMA_TO_DEVICE);

     // Now dma_addr contains the DMA bus address for the buffer, and it can be used for DMA transfers.
     ```

2. **`dma_unmap_single` Function**:

   - `dma_unmap_single` is used to unmap a single buffer that was previously mapped using `dma_map_single`.

   - **Parameters**:
     - `dev`: A pointer to the `struct device` associated with the device for which DMA was performed. This ensures that the memory is unmapped correctly.

     - `handle`: The DMA bus address that was returned when the buffer was mapped using `dma_map_single`.

     - `size`: The size of the buffer in bytes that was mapped.

     - `dir`: The same data direction enum used during mapping, specifying the data direction of the DMA transfers.

   - **Usage Example**:
     ```c
     #include <linux/dma-mapping.h>

     struct device *my_device;  // Pointer to your device structure
     dma_addr_t dma_addr;       // DMA bus address
     size_t buffer_size;        // Size of the buffer

     // Assuming dma_addr and buffer_size are initialized
     dma_unmap_single(my_device, dma_addr, buffer_size, DMA_TO_DEVICE);

     // The buffer is now unmapped and can be safely accessed by the CPU.
     ```

These functions are essential for device drivers that need to perform DMA transfers in the Linux kernel. They handle the complexities of mapping and unmapping memory regions for DMA, ensuring that the memory is suitable for DMA operations and properly synchronized with the CPU's cache.


# dma-mapping: Setting up streaming memory mappings (multiples) 

Scatter and gather are techniques used in computer science and data transfer to optimize memory access patterns and data movement. They are particularly relevant in the context of DMA (Direct Memory Access) operations and I/O.

1. **Scatter**:
   - Scatter refers to the process of breaking a contiguous block of data into smaller, non-contiguous chunks. Each chunk may be stored in a different memory location.
   - This technique is used when data from a single source needs to be distributed or scattered to multiple destinations, each possibly at a different memory location.
   - Scatter is often used in DMA transfers where data is sourced from multiple buffers or locations and needs to be sent to different locations in memory.

2. **Gather**:
   - Gather is the opposite of scatter. It refers to collecting data from multiple non-contiguous sources and gathering them into a contiguous block or buffer.
   - Gather is used when data from multiple sources, each possibly at different memory locations, needs to be combined or collected into a single destination buffer.
   - In the context of DMA, gather can be seen when data is collected from various memory locations and brought together for further processing or transmission.

The code below  provided demonstrates the use of scatter in DMA operations using the scatter-gather library in the Linux kernel. Here's a breakdown of the code:

```c 

#include <linux/dma-mapping.h>  // Include the DMA mapping header
#include <linux/scatterlist.h>  // Include the scatterlist header

/	/ Define the number of scatterlist entries you want to use
#define NENTS 2

// Declare variables
struct scatterlist sglist[NENTS], *sg;  // Scatterlist array
int i, count;  // Loop counters and count variable

// Initialize the scatterlist table with NENTS entries
sg_init_table(sglist, NENTS);

// Set up the first scatterlist entry (sglist[0]) with buffer buf0 and its length len0
sg_set_buf(&sglist[0], buf0, len0);

// Set up the second scatterlist entry (sglist[1]) with buffer buf1 and its length len1
sg_set_buf(&sglist[1], buf1, len1);

// Map the scatterlist entries for DMA transfers in DMA_TO_DEVICE direction
count = dma_map_sg(dev, sglist, NENTS, DMA_TO_DEVICE);

// Loop through the mapped scatterlist entries to extract DMA addresses and lengths
for_each_sg(sglist, sg, count, i) {
    dma_address[i] = sg_dma_address(sg);  // Get the DMA address of the scatterlist entry
    dma_len[i] = sg_dma_len(sg);  // Get the length of the scatterlist entry
}

// ... Perform DMA operations using the mapped scatterlist entries ...

// Unmap the scatterlist entries when they are no longer needed for DMA transfers
dma_unmap_sg(dev, sglist, count, DMA_TO_DEVICE);

```

- `struct scatterlist sglist[NENTS], *sg;`: This declares an array of scatterlist structures. Each scatterlist represents a non-contiguous memory region.

- `sg_init_table(sglist, NENTS);`: Initializes the scatterlist array.

- `sg_set_buf(&sglist[0], buf0, len0);` and `sg_set_buf(&sglist[1], buf1, len1);`: These lines set up two scatterlist entries, `sglist[0]` and `sglist[1]`, with different memory buffers `buf0` and `buf1` along with their respective lengths.

- `count = dma_map_sg(dev, sglist, NENTS, DMA_TO_DEVICE);`: This function maps the scatterlist entries for DMA transfers and returns the count of scatterlist entries that were mapped.

- `for_each_sg(sglist, sg, count, i) { ... }`: This loop iterates through the mapped scatterlist entries and extracts information about each entry, such as the DMA address and length.

- `dma_unmap_sg(dev, sglist, count, DMA_TO_DEVICE);`: Finally, this function unmaps the scatterlist entries when they are no longer needed for DMA transfers.

In this code, scatter is used to map data from different buffers (`buf0` and `buf1`) into separate scatterlist entries, allowing them to be efficiently DMA transferred to different memory locations. This technique helps optimize data movement and memory access patterns in DMA operations.


## Simple DMA Code with SG 

``` c
#include <linux/module.h>
#include <linux/init.h>
#include <linux/platform_device.h>
#include <linux/dma-mapping.h>
#include <linux/scatterlist.h>

// Define the device structure
struct my_dma_device {
    struct platform_device *pdev;
    struct dma_chan *dma_channel;
};

// Scatter-gather buffers
static char sg_buf1[128];
static char sg_buf2[256];

// DMA callback function
static void dma_callback(void *data)
{
    pr_info("DMA transfer complete!\n");
}

static int my_dma_probe(struct platform_device *pdev)
{
    struct dma_chan *chan;
    struct scatterlist sglist[2];
    struct dma_async_tx_descriptor *desc;
    struct my_dma_device *mydev;

    pr_info("Probing My DMA Driver...\n");

    // Allocate memory for the device structure
    mydev = devm_kzalloc(&pdev->dev, sizeof(struct my_dma_device), GFP_KERNEL);
    if (!mydev)
        return -ENOMEM;

    mydev->pdev = pdev;

    // Initialize scatter-gather lists
    sg_init_table(sglist, ARRAY_SIZE(sglist));
    sg_set_buf(&sglist[0], sg_buf1, sizeof(sg_buf1));
    sg_set_buf(&sglist[1], sg_buf2, sizeof(sg_buf2));

    // Get a DMA channel
    chan = dma_request_chan(&pdev->dev, "dma-name");
    if (!chan)
        return -ENODEV;

    mydev->dma_channel = chan;

    // Create a DMA transaction descriptor for scatter-gather
    desc = dmaengine_prep_slave_sg(chan, sglist, ARRAY_SIZE(sglist), DMA_MEM_TO_DEV, DMA_PREP_INTERRUPT);
    if (!desc) {
        dma_release_channel(chan);
        return -ENOMEM;
    }

    // Set the callback function
    desc->callback = dma_callback;
    desc->callback_param = mydev;

    // Submit the DMA transfer
    dmaengine_submit(desc);
    dma_async_issue_pending(chan);

    return 0;
}

static int my_dma_remove(struct platform_device *pdev)
{
    struct my_dma_device *mydev = platform_get_drvdata(pdev);

    pr_info("Removing My DMA Driver...\n");

    dmaengine_terminate_all(mydev->dma_channel);
    dma_release_channel(mydev->dma_channel);

    return 0;
}

static struct platform_driver my_dma_driver = {
    .driver = {
        .name = "my-dma-driver",
        .owner = THIS_MODULE,
    },
    .probe = my_dma_probe,
    .remove = my_dma_remove,
};

static int __init my_dma_init(void)
{
    return platform_driver_register(&my_dma_driver);
}

static void __exit my_dma_exit(void)
{
    platform_driver_unregister(&my_dma_driver);
}

module_init(my_dma_init);
module_exit(my_dma_exit);

MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("Simple DMA Driver with Scatter-Gather");
MODULE_LICENSE("GPL");
```
# dma-mapping: Setting up streaming I/O mappings


The `dma_map_resource` and `dma_unmap_resource` functions provided by the Linux kernel's DMA mapping API are used for mapping and unmapping physical memory addresses that may correspond to memory-mapped I/O (MMIO) registers or other hardware resources. These functions ensure that the addresses are properly handled for DMA operations, including remapping when necessary, such as when using an I/O memory management unit (IOMMU).

Here is an explanation of these functions along with their parameters:

1. `dma_map_resource` Function:
   - `struct device *dev`: Pointer to the device structure associated with the DMA operation.
   - `phys_addr_t paddr`: The physical address of the resource to be mapped.
   - `size_t size`: The size (in bytes) of the resource to be mapped.
   - `enum dma_data_direction dir`: Specifies the direction of the DMA transfer (e.g., `DMA_TO_DEVICE`, `DMA_FROM_DEVICE`, or `DMA_BIDIRECTIONAL`).
   - `unsigned long attrs`: Optional attributes to specify additional behavior or flags for the mapping (can be set to 0 for default behavior).

This function maps the specified physical address range into a DMA address that can be used for DMA operations. It ensures that the necessary remapping is performed if required by the platform's IOMMU.

2. `dma_unmap_resource` Function:
   - `struct device *dev`: Pointer to the device structure associated with the DMA operation.
   - `dma_addr_t handle`: The DMA address obtained from a previous `dma_map_resource` call.
   - `size_t size`: The size (in bytes) of the resource being unmapped.
   - `enum dma_data_direction dir`: Specifies the direction of the DMA transfer (must match the direction used in `dma_map_resource`).
   - `unsigned long attrs`: Optional attributes, which should match the attributes used in `dma_map_resource`.

This function unmaps a previously mapped resource. It releases any resources associated with the mapping and performs any necessary cleanup.

Here is an example of how to use these functions:

```c
#include <linux/dma-mapping.h>

struct device *my_device;  // Assuming you have a valid device structure.

phys_addr_t mmio_phys_addr = 0x10000000;  // Replace with the actual MMIO address.
size_t mmio_size = 0x1000;              // Replace with the actual size.

// Map the physical MMIO address for DMA.
dma_addr_t mmio_dma_addr = dma_map_resource(my_device, mmio_phys_addr, mmio_size, DMA_TO_DEVICE, 0);

// Perform DMA operations using mmio_dma_addr.

// Unmap the MMIO resource when done.
dma_unmap_resource(my_device, mmio_dma_addr, mmio_size, DMA_TO_DEVICE, 0);
```

In this example, we map an MMIO resource with a physical address `mmio_phys_addr` and size `mmio_size`. We perform DMA operations using the DMA-mapped address `mmio_dma_addr` and unmap the resource when done.

These functions ensure that the addresses are correctly handled for DMA and take care of any necessary platform-specific remapping or translation.
# dma-mapping: Syncing streaming DMA mappings

The `dma_sync_single_for_cpu`, `dma_sync_single_for_device`, `dma_sync_sg_for_cpu`, and `dma_sync_sg_for_device` functions are used to synchronize the cache and ensure that data is coherent between the CPU and the DMA device when performing DMA operations. These functions are essential when data is being transferred between the CPU and a DMA-capable device, such as a network card or storage controller.

Here's an explanation of these functions along with examples:

1. `dma_sync_single_for_cpu`:
   - `struct device *dev`: Pointer to the device structure associated with the DMA operation.
   - `dma_addr_t dma_handle`: The DMA address obtained from a previous DMA mapping operation.
   - `size_t size`: The size (in bytes) of the data being synchronized.
   - `enum dma_data_direction direction`: Specifies the direction of the DMA transfer (e.g., `DMA_TO_DEVICE` or `DMA_FROM_DEVICE`).

   This function ensures that data transferred to or from the DMA buffer is visible to the CPU. It synchronizes the CPU's cache with the DMA buffer.

   Example:
   ```c
   dma_sync_single_for_cpu(dev, dma_handle, size, DMA_FROM_DEVICE);
   // Now you can safely access the data in the CPU's memory.
   ```

2. `dma_sync_single_for_device`:
   - `struct device *dev`: Pointer to the device structure associated with the DMA operation.
   - `dma_addr_t dma_handle`: The DMA address obtained from a previous DMA mapping operation.
   - `size_t size`: The size (in bytes) of the data being synchronized.
   - `enum dma_data_direction direction`: Specifies the direction of the DMA transfer (e.g., `DMA_TO_DEVICE` or `DMA_FROM_DEVICE`).

   This function ensures that data transferred to or from the DMA buffer is visible to the DMA device. It synchronizes the DMA device's view of the data with the actual data in memory.

   Example:
   ```c
   dma_sync_single_for_device(dev, dma_handle, size, DMA_TO_DEVICE);
   // Now the DMA device can safely access the data.
   ```

3. `dma_sync_sg_for_cpu` and `dma_sync_sg_for_device`:
   These functions are similar to the single-buffer versions but are used for scatter-gather (SG) lists, which are used to map multiple non-contiguous buffers for DMA operations.

   - `struct device *dev`: Pointer to the device structure associated with the DMA operation.
   - `struct scatterlist *sglist`: An array of scatter-gather list entries.
   - `int nents`: The number of entries in the scatter-gather list.
   - `enum dma_data_direction direction`: Specifies the direction of the DMA transfer (e.g., `DMA_TO_DEVICE` or `DMA_FROM_DEVICE`).

   These functions ensure that data in the SG list is coherent between the CPU and the DMA device.

   Example (synchronizing an SG list for the CPU):
   ```c
   dma_sync_sg_for_cpu(dev, sglist, nents, DMA_FROM_DEVICE);
   // Now you can safely access the data in the SG list with the CPU.
   ```

   Example (synchronizing an SG list for a DMA device):
   ```c
   dma_sync_sg_for_device(dev, sglist, nents, DMA_TO_DEVICE);
   // Now the DMA device can safely access the data in the SG list.
   ```

These synchronization functions are crucial for maintaining data integrity when using DMA for data transfers between the CPU and devices. They ensure that the appropriate cache coherency operations are performed, preventing data corruption or inconsistent data views between the CPU and DMA devices.

# Starting DMA transfers

When writing a device driver that relies on an external DMA controller, you'll typically need to interact with the hardware to configure and use DMA for data transfers. The Linux kernel provides the dmaengine framework, including its slave API, to facilitate DMA operations. Here are the steps involved:

1. **Request DMA Channels:** You'll need to request DMA channels from the DMA controller. The specific method for doing this depends on the DMA controller and the hardware platform. This step typically involves interacting with the DMA controller's registers or using platform-specific functions to request DMA channels.

2. **Configure DMA Parameters:** Once you have obtained DMA channels, you'll need to configure various DMA parameters, such as the source and destination addresses, transfer size, direction (DMA_TO_DEVICE or DMA_FROM_DEVICE), burst sizes, and any other relevant parameters. This configuration is typically done through the DMA controller's registers or memory-mapped I/O.

3. **Allocate DMA Buffers:** Allocate memory buffers that are suitable for DMA transfers. These buffers should meet the constraints required by the DMA controller, such as physical contiguity. You can use functions like `dma_alloc_coherent` or `kmalloc` with appropriate flags to allocate DMA-friendly memory.

4. **Map DMA Buffers:** Map the allocated DMA buffers to DMA addresses using functions like `dma_map_single` or `dma_map_sg`. This step ensures that the DMA controller can access the buffers directly without going through the CPU's virtual memory.

5. **Submit DMA Transactions:** Use the DMA channels to submit DMA transactions. Depending on the DMA controller and framework, you may need to use functions provided by the dmaengine framework to submit transactions, which include information about the source and destination addresses, buffer lengths, and direction.

6. **Handle DMA Completion:** Implement appropriate mechanisms to handle DMA completion or errors. You may need to set up interrupt handling or polling to determine when a DMA transaction has completed.

7. **Unmap and Free DMA Buffers:** After DMA transactions are complete, unmap the DMA buffers using functions like `dma_unmap_single` or `dma_unmap_sg`. Then, free the allocated DMA buffers using `dma_free_coherent` or `kfree`.

8. **Release DMA Channels:** When you're done using the DMA channels, release them to make them available for other drivers or components.

The Linux dmaengine framework provides a standardized way to interact with DMA controllers and manage DMA transactions. It abstracts many low-level details and provides a consistent API for device drivers to use DMA efficiently. Device-specific drivers should be developed based on the DMA controller's documentation and the capabilities of the hardware platform.

# The dmaengine framework

The dmaengine framework in the Linux kernel is a subsystem that provides a standardized and unified interface for managing DMA (Direct Memory Access) controllers and performing DMA operations efficiently. It abstracts the complexity of various DMA controllers and provides a consistent API for device drivers to use when working with DMA-enabled hardware. Here are some key features and components of the dmaengine framework:

1. **DMA Engine API:** The dmaengine framework defines a set of APIs and data structures that device drivers can use to interact with DMA controllers. These APIs abstract the low-level details of different DMA controllers and provide a uniform interface.

2. **DMA Channels:** DMA controllers typically have multiple channels that can be used for data transfers. Each channel can be independently configured and operated. The framework allows drivers to request and use DMA channels.

3. **Descriptors:** DMA transactions are described using data structures called descriptors. Each descriptor defines the source and destination addresses, transfer size, direction (DMA_TO_DEVICE or DMA_FROM_DEVICE), and other parameters for a DMA transfer. Device drivers use these descriptors to set up DMA transactions.

4. **Transaction Submission:** Drivers can submit DMA transactions by queuing descriptors to DMA channels. The framework provides functions for submitting single transactions or lists of transactions.

5. **Interrupt Handling:** DMA transactions often generate interrupts upon completion or when certain events occur. The dmaengine framework provides mechanisms for handling these interrupts, allowing drivers to perform actions based on DMA completion or errors.

6. **Buffer Management:** The framework includes functions for mapping and unmapping memory buffers to/from DMA addresses. These functions ensure that DMA transactions can access the correct physical memory locations.

7. **Advanced Features:** DMA controllers may support advanced features like cyclic DMA, scatter-gather (SG) DMA, and cyclic scatter-gather DMA. The dmaengine framework provides support for these features when available in the hardware.

8. **Power Management:** Some DMA controllers support power management features, such as runtime PM (Power Management). The framework includes support for managing the power state of DMA controllers.

9. **DMA Debugging and Tracing:** The framework includes debugging and tracing features to help diagnose issues related to DMA operations and performance.

Device drivers that need to perform DMA operations on various platforms can use the dmaengine framework to ensure portability and efficient use of DMA capabilities. The framework abstracts hardware-specific details, making it easier to write drivers that work across different DMA controller implementations.

Overall, the dmaengine framework plays a crucial role in enabling efficient and reliable DMA operations in the Linux kernel, especially for devices that require high-speed data transfers or complex DMA configurations.


## how device tree, device drivers and dmaengine work  together 


**1. Device Driver and Slave DMA Engine API:**
   - In the context of the DMA engine framework, a device driver interacts with a DMA controller through the DMA engine API.
   - The driver uses the API to retrieve DMA capabilities, configure DMA channels, and perform DMA transactions.
   - Some common operations performed by the device driver include requesting DMA channels, setting up descriptors, submitting DMA transactions, and handling DMA interrupts.

   ```c
   // Example: Requesting a DMA channel
   dma_chan = dma_request_slave_channel(dev, "my_dma_channel");
   
   // Example: Setting up a DMA descriptor
   struct dma_async_tx_descriptor *desc;
   desc = dmaengine_prep_slave_sg(dma_chan, sg_list, num_sg_entries,
                                  DMA_MEM_TO_DEV, DMA_PREP_INTERRUPT);
   
   // Example: Submitting a DMA transaction
   dmaengine_submit(desc);
   dma_async_issue_pending(dma_chan);
   
   // Example: Handling DMA interrupts
   irqreturn_t my_dma_interrupt_handler(int irq, void *dev_id) {
       // Handle DMA completion or error
       // ...
       return IRQ_HANDLED;
   }
   ```

**2. Device Tree Describes DMA Controllers and Channels:**
   - The device tree (DT) is a data structure used in embedded systems to describe hardware configurations.
   - In the context of DMA, the device tree describes DMA controllers and their associated channels.
   - For each DMA controller, the DT specifies properties like compatible device names and base addresses.
   - The DT also indicates which channels are needed for different devices or peripherals that require DMA transfers.
   
   Example (partial Device Tree entry for a DMA controller):
   ```dts
   dma-controller@f0002000 {
       compatible = "my_dma_controller";
       reg = <0xf0002000 0x1000>;
       #dma-cells = <1>;
       dma-channels = <4>;
   };
   ```
   In this example, the DMA controller is described with its compatible name, base address, and the number of channels it has.

**3. Input to the DMA Engine Framework:**
   - Both the device driver and the device tree serve as inputs to the DMA engine framework.
   - The device driver uses the DMA engine API to configure and operate the DMA controller.
   - The device tree provides platform-specific information about the DMA controller's location, properties, and available channels.
   - Together, these inputs help the DMA engine framework manage DMA transactions efficiently.

**4. Output from DMA Engine Framework:**
   - The DMA engine framework provides various callbacks and mechanisms to notify the device driver of DMA completion, errors, or other events.
   - These callbacks allow the device driver to perform actions based on the status of DMA transactions.

   Example (DMA completion callback registration):
   ```c
   struct dma_async_tx_descriptor *desc;
   desc = dmaengine_prep_slave_sg(dma_chan, sg_list, num_sg_entries,
                                  DMA_MEM_TO_DEV, DMA_PREP_INTERRUPT);
   
   desc->callback = my_dma_completion_callback;
   desc->callback_param = my_data;
   
   dmaengine_submit(desc);
   dma_async_issue_pending(dma_chan);
   ```

   In this example, `my_dma_completion_callback` is a callback function provided by the device driver to handle DMA completion events.

 the device driver interacts with the DMA engine framework using the slave DMA engine API. The device tree describes the hardware configuration, including DMA controllers and channels. Together, these inputs enable efficient and platform-specific DMA operations, and the framework provides mechanisms for handling DMA events through callbacks.
## Example 

Creating a Device Tree (DTS) file and using it to configure DMA channels in a device driver with the DMA Slave API requires careful consideration of the specific hardware and platform you are working with. Below is a simplified example of how you might create a Device Tree for a hypothetical platform with two DMA channels and use those channels in a device driver. Please note that this example is a basic illustration 

1. **Create a Device Tree Source (DTS) file:**

Assuming you have a platform with two DMA channels that you want to describe in your DTS file, you can create a simple DTS file like this:

```dts
/dts-v1/;
/plugin/;

/ {
    compatible = "my-platform";

    dma-controller@0 {
        #dma-cells = <1>;  // Each channel has one property
        compatible = "my-dma-controller";

        channel0: dma-channel@0 {
            reg = <0>;
            interrupts = <1 0>;  // Interrupt for channel 0
        };

        channel1: dma-channel@1 {
            reg = <1>;
            interrupts = <2 0>;  // Interrupt for channel 1
        };
    };

    my-device {
        compatible = "my-device";
        /* Other device properties */
    };
};
```

In this example, we define a custom compatible string for your platform and describe two DMA channels (channel0 and channel1) within a `dma-controller` node. Each DMA channel is associated with a specific interrupt.

2. **Device Driver Implementation:**

Next, you would implement a device driver that uses the DMA Slave API to interact with these DMA channels based on the DTS configuration. Below is a simplified example of how you might use these channels:

```c
#include <linux/module.h>
#include <linux/platform_device.h>
#include <linux/dmaengine.h>

static struct dma_chan *dma_ch0, *dma_ch1;

static int my_driver_probe(struct platform_device *pdev) {
    int ret;

    /* Request and configure DMA channels from the Device Tree */
    dma_ch0 = dma_request_slave_channel(&pdev->dev, "channel0");
    if (!dma_ch0) {
        dev_err(&pdev->dev, "Failed to request DMA channel0\n");
        return -ENODEV;
    }

    dma_ch1 = dma_request_slave_channel(&pdev->dev, "channel1");
    if (!dma_ch1) {
        dev_err(&pdev->dev, "Failed to request DMA channel1\n");
        dma_release_channel(dma_ch0);
        return -ENODEV;
    }

    /* Perform DMA operations using dma_ch0 and dma_ch1 */

    return 0;
}

static int my_driver_remove(struct platform_device *pdev) {
    dma_release_channel(dma_ch0);
    dma_release_channel(dma_ch1);
    return 0;
}

static const struct of_device_id my_driver_of_match[] = {
    { .compatible = "my-device" },
    { /* Sentinel */ },
};
MODULE_DEVICE_TABLE(of, my_driver_of_match);

static struct platform_driver my_driver = {
    .probe = my_driver_probe,
    .remove = my_driver_remove,
    .driver = {
        .name = "my-device",
        .of_match_table = my_driver_of_match,
    },
};

module_platform_driver(my_driver);

MODULE_AUTHOR("Your Name");
MODULE_DESCRIPTION("My DMA Driver");
MODULE_LICENSE("GPL");
```

In this device driver example:

- We probe the Device Tree for the DMA channels using `dma_request_slave_channel` based on their names ("channel0" and "channel1").
- The driver can then use these DMA channels for data transfer operations.

Please note that this is a simplified example, and in a real-world scenario, you would need to adapt it to your specific hardware and requirements, taking into consideration the actual hardware capabilities and constraints of your platform. Additionally, you should handle error cases and perform necessary error checking and cleanup operations.


# dmaengine: Slave API: Per-transfer configuration (1/2) 

These following functions  are part of the DMA (Direct Memory Access) API in the Linux kernel and are used to prepare DMA transactions. These functions are typically used when you want to perform data transfers between memory and a device or peripheral using DMA. Let's briefly explain each of them:

1. `dmaengine_prep_slave_single`:
   - This function is used to prepare a single DMA transaction for a slave DMA channel.
   - Parameters:
     - `chan`: A pointer to the DMA channel that you want to use.
     - `buf`: The DMA address of the buffer (source or destination, depending on the `dir` parameter) involved in the transaction.
     - `len`: The length of the data transfer in bytes.
     - `dir`: Specifies the data transfer direction, which can be `DMA_MEM_TO_DEV` (memory to device) or `DMA_DEV_TO_MEM` (device to memory).
     - `flags`: Optional flags that can be used to customize the transaction.

2. `dmaengine_prep_slave_sg`:
   - This function is used to prepare a DMA transaction for a slave DMA channel involving a scatter-gather list (sgl).
   - Parameters:
     - `chan`: A pointer to the DMA channel that you want to use.
     - `sgl`: A pointer to the scatter-gather list describing the data transfer.
     - `sg_len`: The number of entries in the scatter-gather list.
     - `dir`: Specifies the data transfer direction, which can be `DMA_MEM_TO_DEV` (memory to device) or `DMA_DEV_TO_MEM` (device to memory).
     - `flags`: Optional flags that can be used to customize the transaction.

3. `dmaengine_prep_dma_cyclic`:
   - This function is used to prepare a cyclic DMA transaction.
   - Parameters:
     - `chan`: A pointer to the DMA channel that you want to use.
     - `buf`: The DMA address of the buffer (source or destination, depending on the `dir` parameter) involved in the cyclic transaction.
     - `buf_len`: The total length of the buffer in bytes.
     - `period_len`: The length of each cycle in bytes.
     - `dir`: Specifies the data transfer direction, which can be `DMA_MEM_TO_DEV` (memory to device) or `DMA_DEV_TO_MEM` (device to memory).

After preparing the DMA transaction using one of these functions, you typically submit it to the DMA engine for execution. The function returns a descriptor (`struct dma_async_tx_descriptor`) that represents the prepared transaction and can be used to track its progress and completion.

These functions help you set up DMA transactions efficiently, taking into account various parameters such as data direction, buffer addresses, and scatter-gather lists, making it easier to perform high-speed data transfers between memory and devices in a reliable and efficient manner.







The additional flags , `DMA_PREP_INTERRUPT` and `DMA_CTRL_ACK`, are commonly used flags when preparing a DMA transaction with the DMA engine framework in the Linux kernel:

1. **DMA_PREP_INTERRUPT**:
   - Purpose: This flag indicates that an interrupt should be generated when the DMA transaction is completed.
   - Usage: When this flag is set, the DMA controller or engine will generate an interrupt signal to notify the CPU when the DMA transfer is finished. This is often used when you want to asynchronously wait for the completion of the DMA transfer using interrupt-driven programming.

2. **DMA_CTRL_ACK**:
   - Purpose: The `DMA_CTRL_ACK` flag indicates that there's no need for manual acknowledgment (ack) of the transaction by the driver. In other words, the DMA engine automatically handles the acknowledgment of the transaction's completion.
   - Usage: This flag simplifies the driver code by automating the acknowledgment process. Without this flag, the driver might need to explicitly acknowledge the completion of the DMA transaction.

Here's an example of how these flags might be used when preparing a DMA transaction:

```c
struct dma_async_tx_descriptor *tx_desc;

// Prepare a DMA transaction with flags
tx_desc = dmaengine_prep_slave_single(dma_chan, buffer_dma_addr, buffer_size, DMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);

if (tx_desc) {
    // Set a callback function
    tx_desc->callback = my_dma_completion_callback;
    tx_desc->callback_param = my_dev;

    // Submit the DMA transaction for execution
    dmaengine_submit(tx_desc);

    // The transaction is in progress, and an interrupt will be generated upon completion
} else {
    // Handle the case where DMA preparation failed
}
```

In this example, the `DMA_PREP_INTERRUPT` flag is used to request an interrupt upon completion, and the `DMA_CTRL_ACK` flag indicates that manual acknowledgment is not required. These flags can help simplify the driver code when working with DMA transactions.



In DMA programming, a callback function is a user-defined function that is executed when a DMA transfer is complete or when certain conditions are met. It allows the programmer to perform specific actions or processing after a DMA operation has finished. Callback functions are especially useful for asynchronous DMA transfers, where the CPU may continue executing other tasks while the DMA operation is in progress.

The followin code snippet  demonstrates how to set up a callback for a DMA descriptor:

```c
desc->callback = foo_dma_complete;
desc->callback_param = foo_dev;
```

Here's an explanation of each part:

1. `desc->callback`: This line assigns a callback function to the `callback` field of a DMA descriptor (`desc`). The `foo_dma_complete` function is the callback function that will be executed when the DMA transfer associated with this descriptor is complete.

2. `desc->callback_param`: This line sets the `callback_param` field of the DMA descriptor. It allows you to pass a parameter (in this case, `foo_dev`) to the callback function. The parameter can be used by the callback function to access additional data or context relevant to the DMA operation.

In this context, `foo_dma_complete` is expected to be a user-defined function with a specific signature that matches the requirements of the DMA engine or framework being used. The DMA framework will call this function when the associated DMA transfer is complete or when other specified conditions are met.

Here's an example of what the `foo_dma_complete` callback function might look like:

```c
void foo_dma_complete(void *param)
{
    struct my_device *dev = (struct my_device *)param;

    // Perform post-DMA processing here, using 'dev' if needed

    // Optionally, signal completion or take other actions
}
```

In this example, `foo_dma_complete` takes a single parameter (`param`), which is expected to be a pointer to a device-specific data structure (e.g., `struct my_device`) that provides context for the callback. Inside the callback, you can perform any necessary post-DMA processing or signal completion to other parts of your driver or application.

Callbacks are an essential mechanism in DMA programming to handle asynchronous data transfers and respond to transfer completion or other events efficiently. They allow you to decouple the CPU's processing from DMA operations and improve system responsiveness.


# dmaengine: Slave API: Per-transfer configuration (2/2)

The  following steps are  involved in using a DMA engine for submitting and managing DMA transfers. Here's an explanation of each step:

1. **Prepare the DMA Descriptor**: First, you prepare a DMA descriptor (`desc`) that describes the DMA transfer you want to perform. This descriptor includes information such as the source and destination addresses, transfer size, direction, and any relevant flags.

2. **Queue the Next Operation**:
   - `dma_cookie_t cookie;`: This line declares a variable to store a DMA transaction cookie. The cookie is a unique identifier for the submitted DMA operation.
   - `cookie = dmaengine_submit(desc);`: Here, you submit the prepared DMA descriptor to the DMA engine, which queues the operation for execution. The `dmaengine_submit` function returns a cookie that identifies the submitted transaction.
   - `ret = dma_submit_error(cookie);`: You can check if there was an error in submitting the DMA operation by using the `dma_submit_error` function. If `ret` is non-zero, it indicates an error occurred during submission.

3. **Trigger the Queued Transfers**:
   - `dma_async_issue_pending(chan);`: This line triggers the execution of all queued DMA transfers associated with the DMA channel (`chan`). When you call this function, the DMA engine starts processing the pending DMA operations.

3bis. **Terminate Ongoing Transactions** (Optional):
   - `dmaengine_terminate_sync(chan);`: In certain situations, you might want to terminate all ongoing DMA transactions on the DMA channel (`chan`). This function forcefully terminates any in-progress transfers. This step is useful when you need to stop ongoing DMA operations due to an error or when the device should no longer be used.

Overall, these steps allow you to submit DMA transfers to the DMA engine, trigger their execution, and optionally terminate ongoing transactions if needed. DMA engines provide a flexible and efficient way to handle data transfers between memory and devices in a system, especially in scenarios where high data throughput or low CPU intervention is required.

