---
title: 11. Memory Management
---
# Physical and virtual memory  

Virtual memory is managed by the operating system and allows multiple processes to share a common pool of physical memory (RAM) efficiently. Each process has its own virtual address space, and the OS takes care of mapping virtual addresses to physical addresses in RAM. Here's how this conversion between virtual memory and physical memory works with examples:

**1. Virtual Address Space**:
   - Each process has its own virtual address space, which is divided into segments like code, data, and stack.
   - Virtual addresses in this space are used by the process to access memory.

**2. Page Tables**:
   - The operating system maintains a data structure called a page table for each process. The page table contains entries that map virtual addresses to physical addresses.
   - When a process tries to access a virtual address, the CPU uses the page table to translate it into a physical address.

**3. Paging**:
   - Paging is a memory management scheme used to implement virtual memory.
   - Memory is divided into fixed-size pages in both virtual and physical memory.
   - When a process accesses a virtual memory address, the OS determines the page containing that address.

**4. Page Faults**:
   - If the page corresponding to a virtual address is not in physical memory (it's a page fault), the OS intervenes.
   - The OS selects a page from physical memory to be replaced (if necessary) and loads the needed page from secondary storage (e.g., hard drive) into physical memory.
   
**Example**:
Suppose you have a process running a text editor. It has a virtual address space, and the user is typing text into a document. The process needs to access various parts of memory to manipulate the text. Here's how virtual memory works in this scenario:

1. **Virtual Address Request**: When the process tries to access a specific virtual address, like `0x12345678`, the CPU uses the page table to look up the corresponding physical address.

2. **Page Mapping**: The page table entry for `0x12345678` might say that this virtual address corresponds to physical page `0x9ABC`. The OS ensures that page `0x9ABC` is in physical memory (RAM).

3. **Physical Memory Access**: The CPU accesses the data at the physical address `0x9ABC` in RAM, allowing the process to read or modify the text.

4. **Page Fault Handling**: If page `0x9ABC` is not in physical memory (perhaps it was swapped out to secondary storage due to memory pressure), a page fault occurs.

5. **Page Swap**: The OS selects a page to evict from RAM (e.g., an unused page) and swaps it with the needed page from secondary storage. The page containing `0x12345678` is now in RAM.

6. **Retry Access**: The CPU retries the access to `0x12345678`, and this time it succeeds because the page is in physical memory.

This process of virtual-to-physical address translation, page mapping, and handling page faults allows multiple processes to share physical memory efficiently while ensuring data is available when needed. The OS manages this complexity transparently to both the user and the application.


# Physical/virtual memory mapping on 32-bit systems
 
In the Linux kernel's memory management, the concepts of "Zone_HIGHMEM," "Zone_NORMAL," and the virtual address space are important for understanding how the kernel manages physical and virtual memory on systems with limited addressable memory.

**1. Virtual Address Space:**
   - Virtual address space refers to the range of memory addresses that a process or the kernel can use.
   - In a 32-bit system, the virtual address space is typically 4 GB in size.
   - In a 64-bit system, the virtual address space is much larger (e.g., 128 TB), allowing for addressing a vast amount of memory.

**2. Zone_HIGHMEM:**
   - Zone_HIGHMEM is a memory zone in the Linux kernel used to manage memory beyond the direct addressable memory limit of a 32-bit system.
   - On a 32-bit system, the kernel can directly address up to 4 GB of physical memory.
   - If the system has more than 4 GB of RAM, the additional memory is considered "high memory" or "Zone_HIGHMEM."
   - Zone_HIGHMEM is located above the 4 GB boundary and is not directly addressable using 32-bit virtual addresses.
   - To access memory in Zone_HIGHMEM, the kernel must use special techniques, such as temporary mappings or direct I/O, as the memory is not part of the normal address space.

**3. Zone_NORMAL:**
   - Zone_NORMAL, often referred to as the "normal zone," is the portion of physical memory that falls within the 4 GB addressable limit of a 32-bit system.
   - Memory in this zone can be directly accessed using 32-bit virtual addresses.
   - This zone includes both kernel space and user space memory regions.
   - Most of the system's physical memory falls into the Zone_NORMAL category.

**4. Virtual-to-Physical Address Translation:**
   - The Linux kernel uses page tables to translate virtual addresses used by processes and the kernel into physical addresses.
   - The virtual address space is divided into pages, and page tables provide the mapping of virtual pages to physical pages in memory.
   - The page tables are used for both user space and kernel space address translations.

**5. Handling High Memory:**
   - When a 32-bit Linux kernel encounters memory beyond the 4 GB boundary (Zone_HIGHMEM), it must use special techniques to access it.
   - One common approach is temporary mapping, where a portion of Zone_HIGHMEM is mapped into Zone_NORMAL temporarily to perform necessary operations.
   - This mapping is done dynamically as needed and is not a permanent mapping.

In summary, on 32-bit systems, the Linux kernel manages memory using two primary zones: Zone_NORMAL for memory within the 4 GB addressable limit and Zone_HIGHMEM for memory beyond that limit. The kernel uses page tables to translate virtual addresses to physical addresses for both zones, but special techniques are required to access high memory in Zone_HIGHMEM.


## Pros and Cons of High memory usage 

In the Linux kernel's memory management, the concepts of "Zone_HIGHMEM," "Zone_NORMAL," and the virtual address space are important for understanding how the kernel manages physical and virtual memory on systems with limited addressable memory.

**1. Virtual Address Space:**
   - Virtual address space refers to the range of memory addresses that a process or the kernel can use.
   - In a 32-bit system, the virtual address space is typically 4 GB in size.
   - In a 64-bit system, the virtual address space is much larger (e.g., 128 TB), allowing for addressing a vast amount of memory.

**2. Zone_HIGHMEM:**
   - Zone_HIGHMEM is a memory zone in the Linux kernel used to manage memory beyond the direct addressable memory limit of a 32-bit system.
   - On a 32-bit system, the kernel can directly address up to 4 GB of physical memory.
   - If the system has more than 4 GB of RAM, the additional memory is considered "high memory" or "Zone_HIGHMEM."
   - Zone_HIGHMEM is located above the 4 GB boundary and is not directly addressable using 32-bit virtual addresses.
   - To access memory in Zone_HIGHMEM, the kernel must use special techniques, such as temporary mappings or direct I/O, as the memory is not part of the normal address space.

**3. Zone_NORMAL:**
   - Zone_NORMAL, often referred to as the "normal zone," is the portion of physical memory that falls within the 4 GB addressable limit of a 32-bit system.
   - Memory in this zone can be directly accessed using 32-bit virtual addresses.
   - This zone includes both kernel space and user space memory regions.
   - Most of the system's physical memory falls into the Zone_NORMAL category.

**4. Virtual-to-Physical Address Translation:**
   - The Linux kernel uses page tables to translate virtual addresses used by processes and the kernel into physical addresses.
   - The virtual address space is divided into pages, and page tables provide the mapping of virtual pages to physical pages in memory.
   - The page tables are used for both user space and kernel space address translations.

**5. Handling High Memory:**
   - When a 32-bit Linux kernel encounters memory beyond the 4 GB boundary (Zone_HIGHMEM), it must use special techniques to access it.
   - One common approach is temporary mapping, where a portion of Zone_HIGHMEM is mapped into Zone_NORMAL temporarily to perform necessary operations.
   - This mapping is done dynamically as needed and is not a permanent mapping.

In summary, on 32-bit systems, the Linux kernel manages memory using two primary zones: Zone_NORMAL for memory within the 4 GB addressable limit and Zone_HIGHMEM for memory beyond that limit. The kernel uses page tables to translate virtual addresses to physical addresses for both zones, but special techniques are required to access high memory in Zone_HIGHMEM.

# Userspace memory allocations
##  mlock` and `mlockall
`mlock` and `mlockall` are system calls in Linux that are used for locking memory pages in RAM to prevent them from being swapped out to disk by the operating system's memory management system. These calls are often used in real-time and high-performance computing applications where predictable and low-latency memory access is critical.

Here's an explanation of both calls:

1. **mlock**:
   - `mlock` is a system call that locks a specific range of memory pages into RAM, preventing them from being swapped out to disk.
   - It is typically used to ensure that critical data or code remains in physical memory to minimize access latency.
   - The `mlock` function takes three arguments:
     - `addr`: A pointer to the start of the memory range to be locked.
     - `len`: The length (in bytes) of the memory range to be locked.
     - `flags`: Additional flags that can modify the behavior of the call (usually set to 0).

   Example:
   ```c
   #include <sys/mman.h>

   int main() {
       void* addr = /* pointer to allocated memory */;
       size_t len = /* size of memory */;
       
       if (mlock(addr, len) == -1) {
           perror("mlock");
           return 1;
       }

       // Memory is now locked in RAM
       
       // ... Perform operations on locked memory ...

       // Don't forget to eventually unlock the memory when done
       if (munlock(addr, len) == -1) {
           perror("munlock");
           return 1;
       }

       return 0;
   }
   ```

2. **mlockall**:
   - `mlockall` is a system call that locks all of the calling process's address space into RAM.
   - It is used to ensure that the entire address space remains in physical memory, which can be useful for real-time applications.
   - The `mlockall` function takes a single argument, which is a set of flags that control its behavior. The most common flag is `MCL_FUTURE`, which locks all future memory allocations as well.

   Example:
   ```c
   #include <sys/mman.h>

   int main() {
       int flags = MCL_CURRENT | MCL_FUTURE; // Lock existing and future memory

       if (mlockall(flags) == -1) {
           perror("mlockall");
           return 1;
       }

       // All memory allocations will be locked in RAM from now on

       // ... Perform operations on locked memory ...

       // Don't forget to eventually unlock the memory when done
       if (munlockall() == -1) {
           perror("munlockall");
           return 1;
       }

       return 0;
   }
   ```

It's important to use these calls judiciously, as locking too much memory can lead to resource contention and system instability. They should be used when precise control over memory behavior is necessary for performance or real-time requirements.



## OOM (Out of Memory) killer

The OOM (Out of Memory) killer is a feature in the Linux kernel designed to handle situations where the system is running out of physical memory (RAM) and needs to free up memory to continue functioning. When the kernel detects that it's running critically low on memory and all other memory-reclaiming techniques (like swapping) have been exhausted, the OOM killer comes into action.

Here's how the OOM killer works:

1. **Detecting Low Memory:** The Linux kernel continuously monitors the available physical memory. When the available memory drops below a certain threshold, the kernel identifies the situation as a potential OOM condition.

2. **Selecting a Victim:** When the system reaches an OOM condition, the kernel must decide which process to terminate in order to free up memory. It selects a process as a "victim" for termination.

3. **Scoring:** Each process is assigned a score based on various factors, including its memory usage, the amount of time it has been running, and its oom_score_adj value (a user-adjustable score that allows users to influence the OOM killer's decisions). The process with the highest score is chosen as the victim.

4. **Termination:** The selected process is forcefully terminated by the OOM killer. This typically results in the process receiving a SIGKILL signal, causing it to exit immediately.

5. **Recovery:** After killing a process, the OOM killer attempts to free up memory and restore system stability. It may trigger other memory-reclaiming mechanisms, such as swapping or swapping out less critical pages to disk.

The OOM killer is a last-resort mechanism to prevent the system from becoming completely unresponsive due to memory exhaustion. While it helps prevent system crashes, it does so at the cost of terminating user processes, which can lead to data loss or service interruptions.

To manage the OOM killer's behavior, you can adjust the oom_score_adj values for specific processes to influence their chances of being selected as victims. Additionally, you can disable the OOM killer entirely for certain processes using the `OOM_DISABLE` flag.

It's important to configure the OOM killer carefully to strike a balance between system stability and application reliability. In some cases, administrators may also invest in additional hardware or memory management strategies to minimize OOM events.



# Kernel memory allocators

In the Linux kernel, memory allocation and management are crucial tasks. The kernel provides several memory allocation functions to manage various types of memory pools, each tailored for specific purposes. Here are some of the primary memory allocators used in the Linux kernel:

1. **kmalloc and kfree**:
   - `kmalloc` and `kfree` are the most commonly used memory allocation functions in the kernel.
   - `kmalloc` is used to dynamically allocate a specified amount of memory from the kernel's heap.
   - `kfree` is used to release memory previously allocated with `kmalloc`.
   - These functions are suitable for relatively small memory allocations.

   Example:
   ```c
   void *ptr = kmalloc(size, GFP_KERNEL);  // Allocate 'size' bytes of memory
   if (ptr) {
       // Memory allocation successful
       // ...
       kfree(ptr);  // Release memory when no longer needed
   }
   ```

2. **vmalloc and vfree**:
   - `vmalloc` and `vfree` are used for allocating and freeing variable-sized memory regions.
   - `vmalloc` is suitable for larger memory allocations and can be used when physically contiguous memory is not necessary.
   - The memory allocated by `vmalloc` may be scattered throughout physical memory.

   Example:
   ```c
   void *ptr = vmalloc(size);  // Allocate 'size' bytes of memory
   if (ptr) {
       // Memory allocation successful
       // ...
       vfree(ptr);  // Release memory when no longer needed
   }
   ```

3. **get_free_pages and free_pages**:
   - `get_free_pages` allocates a specified number of physically contiguous pages of memory.
   - `free_pages` is used to free memory previously allocated with `get_free_pages`.
   - These functions are typically used for low-level memory management.

   Example:
   ```c
   unsigned long *ptr = (unsigned long *)get_free_pages(GFP_KERNEL, order);
   if (ptr) {
       // Memory allocation successful
       // ...
       free_pages((unsigned long)ptr, order);  // Release memory
   }
   ```

4. **dma_alloc_coherent and dma_free_coherent**:
   - These functions are used for memory allocation and deallocation in the context of Direct Memory Access (DMA) operations.
   - They ensure that the allocated memory is suitable for DMA transfers.

   Example:
   ```c
   void *ptr = dma_alloc_coherent(dev, size, &dma_handle, GFP_KERNEL);
   if (ptr) {
       // DMA-compatible memory allocation successful
       // ...
       dma_free_coherent(dev, size, ptr, dma_handle);  // Release DMA memory
   }
   ```

5. **SLAB and SLOB**:
   - SLAB and SLOB are memory allocators within the kernel that manage memory pools for frequently used data structures.
   - These allocators are designed to reduce fragmentation and improve memory allocation efficiency.

Each of these memory allocation functions has specific use cases and trade-offs. Kernel developers choose the appropriate allocator based on the requirements of their code and the target hardware platform.

## "SLAB allocators" 

In the context of the Linux kernel, "SLAB allocators" refer to a family of memory allocation techniques used to efficiently manage memory for frequently used data structures. The term "SLAB" is derived from the idea of dividing memory into fixed-size "slabs," each of which can be used to allocate objects of a specific size. SLAB allocators are part of the kernel's memory management subsystem and are designed to minimize memory fragmentation and improve memory allocation performance.

Here are some key characteristics and concepts related to SLAB allocators:

1. **Slabs:** A slab is a fixed-size block of memory that is used to store objects of a specific size. Slabs are preallocated during system initialization and are divided into smaller units called "cache lines." Each cache line can hold an object of the desired size. Slabs are organized into caches, with each cache corresponding to a specific object size.

2. **Caches:** A cache is a collection of slabs that hold objects of the same size. Each cache is associated with a specific object type or data structure within the kernel. Caches are created and initialized during system startup, and they are used to efficiently allocate and deallocate objects of the associated size.

3. **Object Allocation:** When a kernel component needs to allocate an object of a particular size, it requests the appropriate cache. The SLAB allocator checks if there are available objects in the cache. If there are, it provides a free object. If the cache is empty, a new slab is allocated and initialized, and objects are provided from that slab.

4. **Object Deallocation:** When an object is no longer needed, it is returned to the cache for reuse. The SLAB allocator keeps track of free objects within slabs and caches.

5. **Efficiency:** SLAB allocators are designed for efficiency. They reduce memory fragmentation by allocating and deallocating objects of the same size from preallocated slabs. This minimizes overhead and fragmentation compared to more general-purpose allocators like `kmalloc` or `vmalloc`.

6. **Cache Reclamation:** SLAB allocators include mechanisms for cache reclamation to release memory when it is no longer needed. This helps prevent memory leaks and ensures efficient memory usage.

7. **Customization:** Kernel developers can define custom caches for specific data structures and specify object sizes. This allows them to optimize memory allocation for different parts of the kernel.

SLAB allocators are particularly useful for managing memory for kernel data structures that have a predictable size and are frequently allocated and deallocated. They are widely used throughout the Linux kernel to efficiently manage memory for various data structures, such as file system structures, networking data, and more.

Overall, SLAB allocators contribute to the performance and stability of the Linux kernel by providing an efficient way to manage memory for commonly used data structures.

## "identity-mapped part of the kernel memory space"

The "identity-mapped part of the kernel memory space" refers to a specific range of virtual memory addresses within the kernel's address space that directly correspond to physical memory addresses. In this context, "identity mapping" means that the virtual addresses are identical to the physical addresses, allowing for direct and efficient memory access.

Here are some key points about the identity-mapped part of the kernel memory space:

1. **Kernel Memory Space:** In an operating system like Linux, the kernel operates in its own dedicated memory space, separate from user space. The kernel's memory space contains its code, data structures, and other essential components required for system operation.

2. **Virtual Memory:** The kernel interacts with memory using virtual memory addresses rather than physical memory addresses. This abstraction allows the kernel to manage and control memory more flexibly and securely.

3. **Identity Mapping:** The identity-mapped part of the kernel memory space is a region where the virtual memory addresses directly correspond to the physical memory addresses. In other words, if you access a virtual memory address within this range, the data stored at that address is located at the same offset in physical memory.

4. **Purpose:** Identity mapping is used for several purposes, including direct access to physical memory, efficient memory management, and low-level hardware interactions. For example, device drivers and certain kernel components may need to interact with hardware peripherals or directly manipulate physical memory, and identity mapping simplifies these tasks.

5. **Physical Memory Access:** When the kernel needs to access physical memory directly, it can do so by using virtual memory addresses within the identity-mapped range. This avoids the need for complex memory management and translation mechanisms.

6. **Kernel Page Tables:** To implement identity mapping, the kernel maintains its own page tables. These page tables ensure that the virtual-to-physical memory mapping remains consistent and efficient.

7. **Security and Privilege:** Access to the identity-mapped part of kernel memory is typically restricted to the kernel itself and trusted components. User-space applications cannot directly access this part of memory, as doing so would pose significant security risks and could lead to system instability.

8. **Specific Ranges:** The identity-mapped part of the kernel memory space is defined by specific address ranges, and the exact ranges may vary depending on the architecture and configuration of the system.

Overall, the identity-mapped part of the kernel memory space provides a means for the kernel to interact with physical memory efficiently and securely. It plays a critical role in the low-level operations of the operating system, such as hardware communication and memory management.

# Page allocator

##   Contiguous Memory Allocator (CMA)

A Contiguous Memory Allocator (CMA) is a memory management technique used in operating systems, particularly in embedded and real-time systems, to allocate contiguous blocks of physical memory. The primary goal of CMA is to provide a contiguous memory region that can be used for various purposes, such as frame buffers, device buffers, or any application that requires physically contiguous memory.

Key characteristics and concepts related to Contiguous Memory Allocator (CMA) include:

1. **Contiguous Memory:** Contiguous memory refers to a block of physical memory in which all the memory addresses are adjacent to each other. In contrast, non-contiguous memory may consist of scattered memory regions with gaps in between.

2. **Embedded Systems:** CMA is commonly used in embedded systems where memory fragmentation can be a concern. It ensures that a contiguous block of memory is available for specific tasks, such as video processing, graphics rendering, or DMA (Direct Memory Access) transfers.

3. **Dynamic Allocation:** CMA dynamically allocates and manages the contiguous memory region based on the system's requirements. It allows for flexible memory allocation and deallocation as needed.

4. **Kernel Support:** To implement CMA, the operating system kernel must provide support for managing and allocating contiguous memory. This often involves kernel-level APIs for requesting and releasing CMA memory.

5. **DMA Buffers:** One of the primary use cases for CMA is to provide physically contiguous memory buffers for DMA operations. Many hardware devices, such as graphics cards or network interfaces, require contiguous memory for efficient data transfers.

6. **Memory Pools:** CMA typically maintains a pool of physically contiguous memory blocks. These blocks are allocated to applications or device drivers when requested and returned to the pool when no longer needed.

7. **Fragmentation Avoidance:** CMA helps avoid memory fragmentation issues that can occur over time as memory is allocated and deallocated. By providing contiguous memory, it ensures that there are no gaps between memory allocations.

8. **Configuration:** The size of the CMA region and its location within physical memory can often be configured based on system requirements. This allows system administrators to optimize memory usage.

9. **Buffer Sharing:** CMA can facilitate sharing of memory buffers between different hardware components or applications. For example, a camera module and a graphics processing unit (GPU) may share a CMA buffer for efficient data transfer.

10. **Performance:** Contiguous memory is often required for high-performance tasks, such as real-time video processing or multimedia playback, where data must be processed sequentially without interruptions.

11. **Kernel Device Drivers:** Kernel device drivers often use CMA to allocate buffers for hardware components that require contiguous memory, such as video encoders, decoders, and graphics accelerators.

12. **CMA API:** The Linux kernel, for example, provides a CMA API that allows device drivers and applications to request and release contiguous memory blocks.

CMA is a valuable memory management technique in scenarios where contiguous memory is crucial for efficient hardware interactions and real-time processing. It ensures that memory fragmentation does not degrade system performance and can be particularly beneficial in embedded and resource-constrained systems.

