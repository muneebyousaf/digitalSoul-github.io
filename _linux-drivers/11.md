---
title: 11. Memory Management
---
# Physical and virtual memory  

Virtual memory is managed by the operating system and allows multiple processes to share a common pool of physical memory (RAM) efficiently. Each process has its own virtual address space, and the OS takes care of mapping virtual addresses to physical addresses in RAM. Here's how this conversion between virtual memory and physical memory works with examples:

**1. Virtual Address Space**:
   - Each process has its own virtual address space, which is divided into segments like code, data, and stack.
   - Virtual addresses in this space are used by the process to access memory.

**2. Page Tables**:
   - The operating system maintains a data structure called a page table for each process. The page table contains entries that map virtual addresses to physical addresses.
   - When a process tries to access a virtual address, the CPU uses the page table to translate it into a physical address.

**3. Paging**:
   - Paging is a memory management scheme used to implement virtual memory.
   - Memory is divided into fixed-size pages in both virtual and physical memory.
   - When a process accesses a virtual memory address, the OS determines the page containing that address.

**4. Page Faults**:
   - If the page corresponding to a virtual address is not in physical memory (it's a page fault), the OS intervenes.
   - The OS selects a page from physical memory to be replaced (if necessary) and loads the needed page from secondary storage (e.g., hard drive) into physical memory.
   
**Example**:
Suppose you have a process running a text editor. It has a virtual address space, and the user is typing text into a document. The process needs to access various parts of memory to manipulate the text. Here's how virtual memory works in this scenario:

1. **Virtual Address Request**: When the process tries to access a specific virtual address, like `0x12345678`, the CPU uses the page table to look up the corresponding physical address.

2. **Page Mapping**: The page table entry for `0x12345678` might say that this virtual address corresponds to physical page `0x9ABC`. The OS ensures that page `0x9ABC` is in physical memory (RAM).

3. **Physical Memory Access**: The CPU accesses the data at the physical address `0x9ABC` in RAM, allowing the process to read or modify the text.

4. **Page Fault Handling**: If page `0x9ABC` is not in physical memory (perhaps it was swapped out to secondary storage due to memory pressure), a page fault occurs.

5. **Page Swap**: The OS selects a page to evict from RAM (e.g., an unused page) and swaps it with the needed page from secondary storage. The page containing `0x12345678` is now in RAM.

6. **Retry Access**: The CPU retries the access to `0x12345678`, and this time it succeeds because the page is in physical memory.

This process of virtual-to-physical address translation, page mapping, and handling page faults allows multiple processes to share physical memory efficiently while ensuring data is available when needed. The OS manages this complexity transparently to both the user and the application.


# Physical/virtual memory mapping on 32-bit systems
 
In the Linux kernel's memory management, the concepts of "Zone_HIGHMEM," "Zone_NORMAL," and the virtual address space are important for understanding how the kernel manages physical and virtual memory on systems with limited addressable memory.

**1. Virtual Address Space:**
   - Virtual address space refers to the range of memory addresses that a process or the kernel can use.
   - In a 32-bit system, the virtual address space is typically 4 GB in size.
   - In a 64-bit system, the virtual address space is much larger (e.g., 128 TB), allowing for addressing a vast amount of memory.

**2. Zone_HIGHMEM:**
   - Zone_HIGHMEM is a memory zone in the Linux kernel used to manage memory beyond the direct addressable memory limit of a 32-bit system.
   - On a 32-bit system, the kernel can directly address up to 4 GB of physical memory.
   - If the system has more than 4 GB of RAM, the additional memory is considered "high memory" or "Zone_HIGHMEM."
   - Zone_HIGHMEM is located above the 4 GB boundary and is not directly addressable using 32-bit virtual addresses.
   - To access memory in Zone_HIGHMEM, the kernel must use special techniques, such as temporary mappings or direct I/O, as the memory is not part of the normal address space.

**3. Zone_NORMAL:**
   - Zone_NORMAL, often referred to as the "normal zone," is the portion of physical memory that falls within the 4 GB addressable limit of a 32-bit system.
   - Memory in this zone can be directly accessed using 32-bit virtual addresses.
   - This zone includes both kernel space and user space memory regions.
   - Most of the system's physical memory falls into the Zone_NORMAL category.

**4. Virtual-to-Physical Address Translation:**
   - The Linux kernel uses page tables to translate virtual addresses used by processes and the kernel into physical addresses.
   - The virtual address space is divided into pages, and page tables provide the mapping of virtual pages to physical pages in memory.
   - The page tables are used for both user space and kernel space address translations.

**5. Handling High Memory:**
   - When a 32-bit Linux kernel encounters memory beyond the 4 GB boundary (Zone_HIGHMEM), it must use special techniques to access it.
   - One common approach is temporary mapping, where a portion of Zone_HIGHMEM is mapped into Zone_NORMAL temporarily to perform necessary operations.
   - This mapping is done dynamically as needed and is not a permanent mapping.

In summary, on 32-bit systems, the Linux kernel manages memory using two primary zones: Zone_NORMAL for memory within the 4 GB addressable limit and Zone_HIGHMEM for memory beyond that limit. The kernel uses page tables to translate virtual addresses to physical addresses for both zones, but special techniques are required to access high memory in Zone_HIGHMEM.


## Pros and Cons of High memory usage 

In the Linux kernel's memory management, the concepts of "Zone_HIGHMEM," "Zone_NORMAL," and the virtual address space are important for understanding how the kernel manages physical and virtual memory on systems with limited addressable memory.

**1. Virtual Address Space:**
   - Virtual address space refers to the range of memory addresses that a process or the kernel can use.
   - In a 32-bit system, the virtual address space is typically 4 GB in size.
   - In a 64-bit system, the virtual address space is much larger (e.g., 128 TB), allowing for addressing a vast amount of memory.

**2. Zone_HIGHMEM:**
   - Zone_HIGHMEM is a memory zone in the Linux kernel used to manage memory beyond the direct addressable memory limit of a 32-bit system.
   - On a 32-bit system, the kernel can directly address up to 4 GB of physical memory.
   - If the system has more than 4 GB of RAM, the additional memory is considered "high memory" or "Zone_HIGHMEM."
   - Zone_HIGHMEM is located above the 4 GB boundary and is not directly addressable using 32-bit virtual addresses.
   - To access memory in Zone_HIGHMEM, the kernel must use special techniques, such as temporary mappings or direct I/O, as the memory is not part of the normal address space.

**3. Zone_NORMAL:**
   - Zone_NORMAL, often referred to as the "normal zone," is the portion of physical memory that falls within the 4 GB addressable limit of a 32-bit system.
   - Memory in this zone can be directly accessed using 32-bit virtual addresses.
   - This zone includes both kernel space and user space memory regions.
   - Most of the system's physical memory falls into the Zone_NORMAL category.

**4. Virtual-to-Physical Address Translation:**
   - The Linux kernel uses page tables to translate virtual addresses used by processes and the kernel into physical addresses.
   - The virtual address space is divided into pages, and page tables provide the mapping of virtual pages to physical pages in memory.
   - The page tables are used for both user space and kernel space address translations.

**5. Handling High Memory:**
   - When a 32-bit Linux kernel encounters memory beyond the 4 GB boundary (Zone_HIGHMEM), it must use special techniques to access it.
   - One common approach is temporary mapping, where a portion of Zone_HIGHMEM is mapped into Zone_NORMAL temporarily to perform necessary operations.
   - This mapping is done dynamically as needed and is not a permanent mapping.

In summary, on 32-bit systems, the Linux kernel manages memory using two primary zones: Zone_NORMAL for memory within the 4 GB addressable limit and Zone_HIGHMEM for memory beyond that limit. The kernel uses page tables to translate virtual addresses to physical addresses for both zones, but special techniques are required to access high memory in Zone_HIGHMEM.

# Userspace memory allocations
##  mlock` and `mlockall
`mlock` and `mlockall` are system calls in Linux that are used for locking memory pages in RAM to prevent them from being swapped out to disk by the operating system's memory management system. These calls are often used in real-time and high-performance computing applications where predictable and low-latency memory access is critical.

Here's an explanation of both calls:

1. **mlock**:
   - `mlock` is a system call that locks a specific range of memory pages into RAM, preventing them from being swapped out to disk.
   - It is typically used to ensure that critical data or code remains in physical memory to minimize access latency.
   - The `mlock` function takes three arguments:
     - `addr`: A pointer to the start of the memory range to be locked.
     - `len`: The length (in bytes) of the memory range to be locked.
     - `flags`: Additional flags that can modify the behavior of the call (usually set to 0).

   Example:
   ```c
   #include <sys/mman.h>

   int main() {
       void* addr = /* pointer to allocated memory */;
       size_t len = /* size of memory */;
       
       if (mlock(addr, len) == -1) {
           perror("mlock");
           return 1;
       }

       // Memory is now locked in RAM
       
       // ... Perform operations on locked memory ...

       // Don't forget to eventually unlock the memory when done
       if (munlock(addr, len) == -1) {
           perror("munlock");
           return 1;
       }

       return 0;
   }
   ```

2. **mlockall**:
   - `mlockall` is a system call that locks all of the calling process's address space into RAM.
   - It is used to ensure that the entire address space remains in physical memory, which can be useful for real-time applications.
   - The `mlockall` function takes a single argument, which is a set of flags that control its behavior. The most common flag is `MCL_FUTURE`, which locks all future memory allocations as well.

   Example:
   ```c
   #include <sys/mman.h>

   int main() {
       int flags = MCL_CURRENT | MCL_FUTURE; // Lock existing and future memory

       if (mlockall(flags) == -1) {
           perror("mlockall");
           return 1;
       }

       // All memory allocations will be locked in RAM from now on

       // ... Perform operations on locked memory ...

       // Don't forget to eventually unlock the memory when done
       if (munlockall() == -1) {
           perror("munlockall");
           return 1;
       }

       return 0;
   }
   ```

It's important to use these calls judiciously, as locking too much memory can lead to resource contention and system instability. They should be used when precise control over memory behavior is necessary for performance or real-time requirements.



## OOM (Out of Memory) killer

The OOM (Out of Memory) killer is a feature in the Linux kernel designed to handle situations where the system is running out of physical memory (RAM) and needs to free up memory to continue functioning. When the kernel detects that it's running critically low on memory and all other memory-reclaiming techniques (like swapping) have been exhausted, the OOM killer comes into action.

Here's how the OOM killer works:

1. **Detecting Low Memory:** The Linux kernel continuously monitors the available physical memory. When the available memory drops below a certain threshold, the kernel identifies the situation as a potential OOM condition.

2. **Selecting a Victim:** When the system reaches an OOM condition, the kernel must decide which process to terminate in order to free up memory. It selects a process as a "victim" for termination.

3. **Scoring:** Each process is assigned a score based on various factors, including its memory usage, the amount of time it has been running, and its oom_score_adj value (a user-adjustable score that allows users to influence the OOM killer's decisions). The process with the highest score is chosen as the victim.

4. **Termination:** The selected process is forcefully terminated by the OOM killer. This typically results in the process receiving a SIGKILL signal, causing it to exit immediately.

5. **Recovery:** After killing a process, the OOM killer attempts to free up memory and restore system stability. It may trigger other memory-reclaiming mechanisms, such as swapping or swapping out less critical pages to disk.

The OOM killer is a last-resort mechanism to prevent the system from becoming completely unresponsive due to memory exhaustion. While it helps prevent system crashes, it does so at the cost of terminating user processes, which can lead to data loss or service interruptions.

To manage the OOM killer's behavior, you can adjust the oom_score_adj values for specific processes to influence their chances of being selected as victims. Additionally, you can disable the OOM killer entirely for certain processes using the `OOM_DISABLE` flag.

It's important to configure the OOM killer carefully to strike a balance between system stability and application reliability. In some cases, administrators may also invest in additional hardware or memory management strategies to minimize OOM events.



# Kernel memory allocators

In the Linux kernel, memory allocation and management are crucial tasks. The kernel provides several memory allocation functions to manage various types of memory pools, each tailored for specific purposes. Here are some of the primary memory allocators used in the Linux kernel:

1. **kmalloc and kfree**:
   - `kmalloc` and `kfree` are the most commonly used memory allocation functions in the kernel.
   - `kmalloc` is used to dynamically allocate a specified amount of memory from the kernel's heap.
   - `kfree` is used to release memory previously allocated with `kmalloc`.
   - These functions are suitable for relatively small memory allocations.

   Example:
   ```c
   void *ptr = kmalloc(size, GFP_KERNEL);  // Allocate 'size' bytes of memory
   if (ptr) {
       // Memory allocation successful
       // ...
       kfree(ptr);  // Release memory when no longer needed
   }
   ```

2. **vmalloc and vfree**:
   - `vmalloc` and `vfree` are used for allocating and freeing variable-sized memory regions.
   - `vmalloc` is suitable for larger memory allocations and can be used when physically contiguous memory is not necessary.
   - The memory allocated by `vmalloc` may be scattered throughout physical memory.

   Example:
   ```c
   void *ptr = vmalloc(size);  // Allocate 'size' bytes of memory
   if (ptr) {
       // Memory allocation successful
       // ...
       vfree(ptr);  // Release memory when no longer needed
   }
   ```

3. **get_free_pages and free_pages**:
   - `get_free_pages` allocates a specified number of physically contiguous pages of memory.
   - `free_pages` is used to free memory previously allocated with `get_free_pages`.
   - These functions are typically used for low-level memory management.

   Example:
   ```c
   unsigned long *ptr = (unsigned long *)get_free_pages(GFP_KERNEL, order);
   if (ptr) {
       // Memory allocation successful
       // ...
       free_pages((unsigned long)ptr, order);  // Release memory
   }
   ```

4. **dma_alloc_coherent and dma_free_coherent**:
   - These functions are used for memory allocation and deallocation in the context of Direct Memory Access (DMA) operations.
   - They ensure that the allocated memory is suitable for DMA transfers.

   Example:
   ```c
   void *ptr = dma_alloc_coherent(dev, size, &dma_handle, GFP_KERNEL);
   if (ptr) {
       // DMA-compatible memory allocation successful
       // ...
       dma_free_coherent(dev, size, ptr, dma_handle);  // Release DMA memory
   }
   ```

5. **SLAB and SLOB**:
   - SLAB and SLOB are memory allocators within the kernel that manage memory pools for frequently used data structures.
   - These allocators are designed to reduce fragmentation and improve memory allocation efficiency.

Each of these memory allocation functions has specific use cases and trade-offs. Kernel developers choose the appropriate allocator based on the requirements of their code and the target hardware platform.

## "SLAB allocators" 

In the context of the Linux kernel, "SLAB allocators" refer to a family of memory allocation techniques used to efficiently manage memory for frequently used data structures. The term "SLAB" is derived from the idea of dividing memory into fixed-size "slabs," each of which can be used to allocate objects of a specific size. SLAB allocators are part of the kernel's memory management subsystem and are designed to minimize memory fragmentation and improve memory allocation performance.

Here are some key characteristics and concepts related to SLAB allocators:

1. **Slabs:** A slab is a fixed-size block of memory that is used to store objects of a specific size. Slabs are preallocated during system initialization and are divided into smaller units called "cache lines." Each cache line can hold an object of the desired size. Slabs are organized into caches, with each cache corresponding to a specific object size.

2. **Caches:** A cache is a collection of slabs that hold objects of the same size. Each cache is associated with a specific object type or data structure within the kernel. Caches are created and initialized during system startup, and they are used to efficiently allocate and deallocate objects of the associated size.

3. **Object Allocation:** When a kernel component needs to allocate an object of a particular size, it requests the appropriate cache. The SLAB allocator checks if there are available objects in the cache. If there are, it provides a free object. If the cache is empty, a new slab is allocated and initialized, and objects are provided from that slab.

4. **Object Deallocation:** When an object is no longer needed, it is returned to the cache for reuse. The SLAB allocator keeps track of free objects within slabs and caches.

5. **Efficiency:** SLAB allocators are designed for efficiency. They reduce memory fragmentation by allocating and deallocating objects of the same size from preallocated slabs. This minimizes overhead and fragmentation compared to more general-purpose allocators like `kmalloc` or `vmalloc`.

6. **Cache Reclamation:** SLAB allocators include mechanisms for cache reclamation to release memory when it is no longer needed. This helps prevent memory leaks and ensures efficient memory usage.

7. **Customization:** Kernel developers can define custom caches for specific data structures and specify object sizes. This allows them to optimize memory allocation for different parts of the kernel.

SLAB allocators are particularly useful for managing memory for kernel data structures that have a predictable size and are frequently allocated and deallocated. They are widely used throughout the Linux kernel to efficiently manage memory for various data structures, such as file system structures, networking data, and more.

Overall, SLAB allocators contribute to the performance and stability of the Linux kernel by providing an efficient way to manage memory for commonly used data structures.

## "identity-mapped part of the kernel memory space"

The "identity-mapped part of the kernel memory space" refers to a specific range of virtual memory addresses within the kernel's address space that directly correspond to physical memory addresses. In this context, "identity mapping" means that the virtual addresses are identical to the physical addresses, allowing for direct and efficient memory access.

Here are some key points about the identity-mapped part of the kernel memory space:

1. **Kernel Memory Space:** In an operating system like Linux, the kernel operates in its own dedicated memory space, separate from user space. The kernel's memory space contains its code, data structures, and other essential components required for system operation.

2. **Virtual Memory:** The kernel interacts with memory using virtual memory addresses rather than physical memory addresses. This abstraction allows the kernel to manage and control memory more flexibly and securely.

3. **Identity Mapping:** The identity-mapped part of the kernel memory space is a region where the virtual memory addresses directly correspond to the physical memory addresses. In other words, if you access a virtual memory address within this range, the data stored at that address is located at the same offset in physical memory.

4. **Purpose:** Identity mapping is used for several purposes, including direct access to physical memory, efficient memory management, and low-level hardware interactions. For example, device drivers and certain kernel components may need to interact with hardware peripherals or directly manipulate physical memory, and identity mapping simplifies these tasks.

5. **Physical Memory Access:** When the kernel needs to access physical memory directly, it can do so by using virtual memory addresses within the identity-mapped range. This avoids the need for complex memory management and translation mechanisms.

6. **Kernel Page Tables:** To implement identity mapping, the kernel maintains its own page tables. These page tables ensure that the virtual-to-physical memory mapping remains consistent and efficient.

7. **Security and Privilege:** Access to the identity-mapped part of kernel memory is typically restricted to the kernel itself and trusted components. User-space applications cannot directly access this part of memory, as doing so would pose significant security risks and could lead to system instability.

8. **Specific Ranges:** The identity-mapped part of the kernel memory space is defined by specific address ranges, and the exact ranges may vary depending on the architecture and configuration of the system.

Overall, the identity-mapped part of the kernel memory space provides a means for the kernel to interact with physical memory efficiently and securely. It plays a critical role in the low-level operations of the operating system, such as hardware communication and memory management.

# Page allocator

##   Contiguous Memory Allocator (CMA)

A Contiguous Memory Allocator (CMA) is a memory management technique used in operating systems, particularly in embedded and real-time systems, to allocate contiguous blocks of physical memory. The primary goal of CMA is to provide a contiguous memory region that can be used for various purposes, such as frame buffers, device buffers, or any application that requires physically contiguous memory.

Key characteristics and concepts related to Contiguous Memory Allocator (CMA) include:

1. **Contiguous Memory:** Contiguous memory refers to a block of physical memory in which all the memory addresses are adjacent to each other. In contrast, non-contiguous memory may consist of scattered memory regions with gaps in between.

2. **Embedded Systems:** CMA is commonly used in embedded systems where memory fragmentation can be a concern. It ensures that a contiguous block of memory is available for specific tasks, such as video processing, graphics rendering, or DMA (Direct Memory Access) transfers.

3. **Dynamic Allocation:** CMA dynamically allocates and manages the contiguous memory region based on the system's requirements. It allows for flexible memory allocation and deallocation as needed.

4. **Kernel Support:** To implement CMA, the operating system kernel must provide support for managing and allocating contiguous memory. This often involves kernel-level APIs for requesting and releasing CMA memory.

5. **DMA Buffers:** One of the primary use cases for CMA is to provide physically contiguous memory buffers for DMA operations. Many hardware devices, such as graphics cards or network interfaces, require contiguous memory for efficient data transfers.

6. **Memory Pools:** CMA typically maintains a pool of physically contiguous memory blocks. These blocks are allocated to applications or device drivers when requested and returned to the pool when no longer needed.

7. **Fragmentation Avoidance:** CMA helps avoid memory fragmentation issues that can occur over time as memory is allocated and deallocated. By providing contiguous memory, it ensures that there are no gaps between memory allocations.

8. **Configuration:** The size of the CMA region and its location within physical memory can often be configured based on system requirements. This allows system administrators to optimize memory usage.

9. **Buffer Sharing:** CMA can facilitate sharing of memory buffers between different hardware components or applications. For example, a camera module and a graphics processing unit (GPU) may share a CMA buffer for efficient data transfer.

10. **Performance:** Contiguous memory is often required for high-performance tasks, such as real-time video processing or multimedia playback, where data must be processed sequentially without interruptions.

11. **Kernel Device Drivers:** Kernel device drivers often use CMA to allocate buffers for hardware components that require contiguous memory, such as video encoders, decoders, and graphics accelerators.

12. **CMA API:** The Linux kernel, for example, provides a CMA API that allows device drivers and applications to request and release contiguous memory blocks.

CMA is a valuable memory management technique in scenarios where contiguous memory is crucial for efficient hardware interactions and real-time processing. It ensures that memory fragmentation does not degrade system performance and can be particularly beneficial in embedded and resource-constrained systems.

# Page allocator API: free pages
##  `get_zeroed_page()`
In the Linux kernel, `get_zeroed_page()` is a function that allocates a single page (usually 4 KB in size) of physical memory and initializes it to all zeros. This function is part of the kernel's memory management subsystem and is commonly used when a kernel module or driver needs to allocate a page of memory for various purposes.

Here's a brief explanation of how `get_zeroed_page()` works:

1. **Memory Allocation:** When you call `get_zeroed_page()`, it requests a single page of physical memory from the kernel's memory allocator. The memory is typically allocated from the free memory pool. The gfp_mask argument specifies the memory allocation flags and constraints, such as whether it can sleep, perform I/O, or use high-priority allocation methods.

2. **Initialization:** After the memory allocation is successful, the function initializes all the bytes in the allocated page to zero. This ensures that the memory is in a consistent and clean state, with no residual data.

3. **Return Value:** The function returns a pointer to the first byte of the allocated and zeroed page. You can use this pointer to access and manipulate the allocated memory.

4. **Error Handling:** If the memory allocation fails (e.g., due to insufficient available memory), `get_zeroed_page()` returns `NULL` to indicate the failure.

Here's a simplified example of how you might use `get_zeroed_page()` in a Linux kernel module:

```c
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/gfp.h>
#include <linux/mm.h>

static void *my_buffer;

static int my_module_init(void)
{
    // Allocate a zeroed page of memory
    my_buffer = (void *)get_zeroed_page(GFP_KERNEL);
    
    if (!my_buffer) {
        printk(KERN_ERR "Memory allocation failed\n");
        return -ENOMEM; // Return an error code
    }

    // Now 'my_buffer' points to a 4 KB page of zeroed memory

    // Perform your operations with 'my_buffer' here

    return 0; // Initialization was successful
}

static void my_module_exit(void)
{
    // Release the allocated memory page
    if (my_buffer)
        free_page((unsigned long)my_buffer);
}

module_init(my_module_init);
module_exit(my_module_exit);
MODULE_LICENSE("GPL");
```

In this example, the `get_zeroed_page()` function is used to allocate a zeroed page of memory, and the allocated memory is later released using `free_page()` in the module's exit function. The allocated memory can be used for various purposes, such as data storage or as a buffer for device operations.



## `__get_free_pages`

The `__get_free_pages(gfp_t gfp_mask, unsigned int order)` function in the Linux kernel is used to allocate a range of physically contiguous pages of memory. Unlike `get_zeroed_page()`, which allocates a single page, `__get_free_pages()` can allocate multiple contiguous pages, where the number of pages is determined by the `order` argument.

Here's an explanation of how `__get_free_pages()` works:

1. **Memory Allocation:** When you call `__get_free_pages(gfp_mask, order)`, it requests a range of `2^order` physically contiguous pages from the kernel's memory allocator. The `gfp_mask` argument specifies the memory allocation flags and constraints, while the `order` argument determines the size of the memory block.

2. **Contiguous Allocation:** The allocated memory block consists of `2^order` consecutive physical pages. These pages are guaranteed to be physically contiguous, which is essential for certain device operations and hardware interfaces.

3. **Return Value:** The function returns a pointer to the first byte of the allocated memory block. The size of the block is determined by the `order` argument and is equal to `PAGE_SIZE * 2^order`. You can use this pointer to access and manipulate the allocated memory.

4. **Error Handling:** If the memory allocation fails due to insufficient available memory or other constraints specified by `gfp_mask`, `__get_free_pages()` returns `NULL` to indicate the failure.

Here's a simplified example of how you might use `__get_free_pages()` in a Linux kernel module to allocate a block of contiguous memory:

```c
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/gfp.h>
#include <linux/mm.h>

static unsigned long *my_buffer;

static int my_module_init(void)
{
    unsigned int order = 2; // Allocate a block of 2^2 = 4 contiguous pages (16 KB)
    
    // Allocate a range of contiguous pages
    my_buffer = (unsigned long *)__get_free_pages(GFP_KERNEL, order);
    
    if (!my_buffer) {
        printk(KERN_ERR "Memory allocation failed\n");
        return -ENOMEM; // Return an error code
    }

    // Now 'my_buffer' points to a contiguous block of memory

    // Perform your operations with 'my_buffer' here

    return 0; // Initialization was successful
}

static void my_module_exit(void)
{
    // Release the allocated memory block
    if (my_buffer)
        free_pages((unsigned long)my_buffer, 2); // Free 4 pages (16 KB)
}

module_init(my_module_init);
module_exit(my_module_exit);
MODULE_LICENSE("GPL");
```

In this example, `__get_free_pages()` is used to allocate a block of four contiguous pages (16 KB in total), and the allocated memory is later released using `free_pages()` when the module is unloaded. The contiguous memory can be beneficial for certain kernel-level data structures and device operations.

## `get_order(size)`
The `get_order(size)` function in the Linux kernel is used to determine the order of a memory allocation based on its size. In the context of memory allocation in the kernel, "order" refers to the exponent of the power of 2 that represents the size of the memory allocation.

Here's how it works:

1. **Input:** You provide the `size` argument, which represents the size of the memory allocation in bytes.

2. **Output:** The function returns the order, which is an integer value indicating the exponent of 2 that corresponds to the size of the memory allocation. In other words, it returns the value `n` such that `size = PAGE_SIZE * (2^n)`.

3. **Usage:** This function is often used when allocating memory with functions like `__get_free_pages()`, which require the order of the allocation. By calculating the order based on the desired size, you can request a specific number of contiguous pages.

Here's a simplified example of how you might use `get_order()` in a Linux kernel module:

```c
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/mm.h>

static void my_module_init(void)
{
    unsigned int size_bytes = 4096; // Allocate 4 KB of memory
    unsigned int order;

    // Calculate the order based on the size
    order = get_order(size_bytes);

    // Output the order and size information
    printk(KERN_INFO "Size: %u bytes, Order: %u\n", size_bytes, order);
}

module_init(my_module_init);
MODULE_LICENSE("GPL");
```

In this example, `get_order()` is used to determine the order required to allocate a 4 KB memory block. It calculates the order as 2 since `4096 = PAGE_SIZE * (2^2)`.

This information can be useful when working with memory allocation functions in the kernel that require the order of the allocation to ensure that the allocated memory is physically contiguous.

# SLAB allocator 1/2

The SLAB allocator is one of the memory management techniques used in the Linux kernel to efficiently allocate and manage memory for objects of the same size. It operates by grouping objects of the same size into caches, which are organized into slabs. Each slab is a fixed-sized block of memory that can hold a specific number of objects of the same size.

Here are some key points about the SLAB allocator:

1. **Caches and Slabs:** The SLAB allocator maintains a set of caches, each associated with a specific object size. These caches are also referred to as "SLAB caches." Each SLAB cache consists of one or more slabs, with each slab containing multiple objects of the same size.

2. **Object Size Variability:** The SLAB allocator can manage objects of varying sizes, including objects smaller than a page or larger than a page. It efficiently handles object sizes that are not multiples of the page size.

3. **Dynamic Management:** The SLAB allocator dynamically manages the number of slabs allocated for each cache based on the demand for objects. If more objects are allocated, it allocates additional slabs as needed. Conversely, if objects are deallocated, it can release empty slabs.

4. **Page Allocator Integration:** The SLAB allocator relies on the kernel's page allocator to allocate and free pages. It requests pages from the page allocator to create new slabs when needed and releases pages when entire slabs become empty.

5. **Common Use Cases:** SLAB caches are used for various data structures that exist in many instances in the kernel. Some examples include directory entries, file objects, network packet descriptors, process descriptors, and more. By grouping objects of the same size into caches, the SLAB allocator reduces fragmentation and improves memory management efficiency.

6. **Performance Benefits:** The SLAB allocator offers performance advantages for frequently allocated and deallocated objects of the same size. It minimizes fragmentation and provides fast access to objects, which is crucial for kernel data structures.

Here's a simplified example of how SLAB caches can be used to manage objects in the Linux kernel:

```c
#include <linux/slab.h>

struct my_struct {
    int data;
    // Other fields
};

void allocate_and_free_objects(void) {
    struct my_struct *obj1, *obj2;

    // Create a SLAB cache for struct my_struct
    struct kmem_cache *my_struct_cache = kmem_cache_create("my_struct_cache", sizeof(struct my_struct), 0, 0, NULL);

    if (!my_struct_cache) {
        printk(KERN_ERR "SLAB cache creation failed\n");
        return;
    }

    // Allocate objects from the SLAB cache
    obj1 = kmem_cache_alloc(my_struct_cache, GFP_KERNEL);
    obj2 = kmem_cache_alloc(my_struct_cache, GFP_KERNEL);

    if (!obj1 || !obj2) {
        printk(KERN_ERR "Memory allocation failed\n");
        goto cleanup;
    }

    // Use the allocated objects

cleanup:
    // Free the objects and destroy the SLAB cache
    kmem_cache_free(my_struct_cache, obj1);
    kmem_cache_free(my_struct_cache, obj2);
    kmem_cache_destroy(my_struct_cache);
}
```

In this example, a SLAB cache named "my_struct_cache" is created for objects of `struct my_struct`. Objects are allocated from the cache using `kmem_cache_alloc()` and deallocated with `kmem_cache_free()`. The SLAB cache is destroyed when it's no longer needed.

The SLAB allocator simplifies memory management for kernel data structures, making it more efficient and reducing the risk of memory fragmentation.

# Different SLAB allocators

The Linux kernel provides three different implementations of a SLAB allocator, which are API compatible, but they differ in terms of complexity, space efficiency, and scalability. These implementations can be chosen at configuration time based on the specific requirements and constraints of the target system. Here's an overview of each implementation:

1. **SLAB Allocator (slab.c):**
   - **Legacy and Well-Proven:** The SLAB allocator is the original SLAB implementation in the Linux kernel. It has been in use for a long time and is well-proven in various scenarios.
   - **Use Cases:** It is suitable for a wide range of systems and use cases.
   - **Legacy Status:** Despite being considered legacy, it is still used in many configurations.
   - **Space Overhead:** It may have slightly higher space overhead compared to the other implementations.
   - **Compatibility:** It is compatible with existing code and configurations.
   - **Example:** Used in 39 defconfig files in Linux 5.10 on ARM (32-bit).

2. **SLOB Allocator (slob.c):**
   - **Simplicity and Space Efficiency:** The SLOB allocator is a much simpler alternative to SLAB. It is designed to be more space-efficient, especially in small systems.
   - **Use Cases:** It can save memory space in resource-constrained environments.
   - **Space Savings:** Depending on the system configuration (CONFIG_EXPERT), it can significantly reduce the kernel's memory footprint.
   - **Trade-offs:** SLOB may not scale well and is not recommended for larger systems.
   - **Example:** Used in 7 defconfig files in Linux 5.10 on ARM (32-bit).
   - **Performance Impact:** On the BeagleBone Black, it resulted in a reduced kernel size of 5 KB and a boot time improvement of 1.43 seconds.

3. **SLUB Allocator (slub.c):**
   - **Simplicity and Scalability:** SLUB is a more recent and simplified implementation compared to SLAB. It offers better scalability and creates less memory fragmentation.
   - **Default Allocator:** SLUB has become the default allocator in the Linux kernel due to its better performance characteristics.
   - **Use Cases:** It is suitable for a wide range of systems and is particularly well-suited for larger systems where scalability is crucial.
   - **Space Overhead:** It may have slightly higher space overhead compared to SLOB but less than SLAB.
   - **Example:** Used in 9 defconfig files in Linux 5.10 on ARM (32-bit).
   - **Performance Impact:** On the BeagleBone Black, it resulted in a larger kernel size (4 KB increase) but only a 2ms increase in total boot time.

The choice of which SLAB allocator to use depends on factors such as the system's available memory, performance requirements, and the desire to optimize kernel size. While SLUB is the default allocator and generally recommended for its scalability and reduced fragmentation, SLAB and SLOB still have their use cases, particularly for legacy systems or those with strict memory constraints.

#  kmalloc allocator

The `kmalloc` allocator is a general-purpose memory allocator in the Linux kernel, and it is widely used for dynamically allocating memory in kernel-space. Here are some key points about `kmalloc`:

1. **Memory Allocation:** `kmalloc` is used to allocate memory for data structures, buffers, and other objects within the kernel.

2. **SLAB and Page Allocator:** Depending on the size of the memory block being allocated, `kmalloc` can use two different mechanisms:
   - For small memory allocations, it relies on the generic SLAB allocator, which uses pre-allocated caches named `kmalloc-XXX` (e.g., `kmalloc-32`, `kmalloc-64`) to efficiently manage small objects.
   - For larger memory allocations, `kmalloc` falls back to using the page allocator.

3. **Contiguous Memory:** One significant advantage of `kmalloc` is that it guarantees that the allocated memory is physically contiguous. This is important for many hardware devices and low-level operations.

4. **Memory Alignment:** The size of the allocated memory block is rounded up to match the size of the smallest SLAB cache that can accommodate it. This helps reduce memory wastage.

5. **GFP Flags:** `kmalloc` uses the same GFP (Get Free Pages) flags as the page allocator, such as `GFP_KERNEL`, `GFP_ATOMIC`, etc. These flags specify memory allocation constraints and behaviors.

6. **Maximum Sizes:** While the maximum size of an individual `kmalloc` allocation varies depending on the architecture (e.g., x86, ARM), it is typically around 4 MB per allocation. The total amount of memory allocated via `kmalloc` is usually capped at around 128 MB.

7. **Primary Allocator:** `kmalloc` is considered the primary memory allocator for most kernel-level memory allocations. It is recommended for general-purpose use unless there is a specific reason to use another allocator.





# kmalloc API 1/2

The `kmalloc` and `kfree` functions are used for memory allocation and deallocation in the Linux kernel. Here's an explanation of their usage along with an example:

1. **`kmalloc` Function:**
   - Prototype: `void *kmalloc(size_t size, gfp_t flags);`
   - Purpose: Allocates a block of memory of the specified size.
   - Parameters:
     - `size`: The number of bytes to allocate.
     - `flags`: GFP flags specifying memory allocation constraints and behaviors, such as `GFP_KERNEL`, `GFP_ATOMIC`, etc.
   - Returns: A pointer to the allocated memory if successful, or `NULL` if memory allocation fails.

2. **`kfree` Function:**
   - Prototype: `void kfree(const void *objp);`
   - Purpose: Releases memory that was previously allocated using `kmalloc`.
   - Parameters:
     - `objp`: A pointer to the memory block to be freed. This should be the same pointer returned by `kmalloc`.

3. **Example:**
   ```c
   #include <linux/slab.h>

   struct ib_port_attr *tprops;

   tprops = kmalloc(sizeof(*tprops), GFP_KERNEL);
   if (!tprops) {
       // Handle allocation failure
       return -ENOMEM;
   }

   // Use the allocated memory (tprops) here...

   kfree(tprops); // Release the allocated memory when done.
   ```

   In this example:
   - We include the `<linux/slab.h>` header to use `kmalloc` and `kfree`.
   - We allocate memory for a structure of type `ib_port_attr` using `kmalloc`. The size of the structure is calculated using `sizeof`.
   - We check if the allocation was successful by verifying that `tprops` is not `NULL`. If allocation fails (due to insufficient memory), we handle the error.
   - We use the allocated memory for some purpose (not shown in the example).
   - Finally, we release the allocated memory using `kfree` when it is no longer needed to avoid memory leaks.

These functions provide a way to allocate and free memory within the kernel, ensuring proper memory management and preventing memory leaks. The choice of GFP flags (e.g., `GFP_KERNEL`) determines the allocation behavior, such as whether the allocation can sleep or must be atomic.

# vmalloc allocator

`vmalloc` allocator is not suitable for DMA (Direct Memory Access) purposes because the memory allocated by `vmalloc` is not guaranteed to be physically contiguous. DMA operations often require physically contiguous memory to transfer data between devices and memory without interruptions. Here's an explanation with an example:

1. **vmalloc Allocator:**
   - The `vmalloc` allocator in the Linux kernel is used to allocate a block of memory in the virtual address space of the kernel.
   - It is suitable for allocating relatively large chunks of memory that are not required to be physically contiguous.
   - `vmalloc` provides a virtual address that may map to non-contiguous physical pages.

2. **DMA Requirements:**
   - DMA controllers in hardware often require physically contiguous memory regions to perform efficient and uninterrupted data transfers between devices (e.g., between a device and main memory).
   - Physically contiguous memory ensures that data can be moved without having to traverse multiple non-contiguous memory regions.

3. **Example:**
   - Let's say you have a network interface card (NIC) that uses DMA to transfer packets between the NIC's memory buffer and the system's main memory.
   - The NIC may require physically contiguous memory buffers for DMA operations to ensure efficient packet transfers.
   - If you use `vmalloc` to allocate memory for the network packet buffers, the memory may not be physically contiguous, leading to performance issues or DMA transfer failures.

Here's a simplified example of why `vmalloc` is not suitable for DMA:

```c
#include <linux/vmalloc.h>

// Allocate a non-contiguous memory region using vmalloc
void *buffer = vmalloc(4096);

// Perform a DMA transfer using the allocated buffer
// The DMA controller expects a physically contiguous buffer

// This DMA transfer may encounter issues due to non-contiguous memory
// or may not perform efficiently, depending on the hardware requirements.

// Free the vmalloc-allocated buffer
vfree(buffer);
```

In the example, the `buffer` allocated by `vmalloc` may not meet the DMA controller's requirements for physically contiguous memory, potentially causing DMA transfer issues.

For DMA purposes, kernel developers typically use other memory allocation methods like `kmalloc` (for smaller physically contiguous allocations) or `get_free_pages` (for larger physically contiguous allocations) to ensure that DMA operations work reliably and efficiently.

# Kernel memory debugging

## KASAN (Kernel Address Sanitizer) 

Kernel Address Sanitizer (KASAN) is a dynamic memory error detector designed for the Linux kernel. It helps identify and diagnose various types of memory-related bugs, such as buffer overflows, use-after-free errors, and invalid memory accesses. KASAN is similar to user-space address sanitizers like AddressSanitizer (ASan) but is adapted for kernel-level code.

Key features and aspects of KASAN:

1. **Memory Error Detection:** KASAN instruments the kernel code to track memory allocations and deallocations. It adds "canaries" or shadow memory to the allocated objects and checks them during memory accesses. If an invalid memory access or corruption is detected, KASAN reports the error.

2. **Types of Bugs Detected:**
   - **Out-of-Bounds Access:** KASAN can detect reads or writes beyond the boundaries of allocated memory.
   - **Use-After-Free:** It can catch attempts to access memory after it has been freed, helping prevent use-after-free vulnerabilities.
   - **Memory Leak Detection:** KASAN can also identify memory leaks by tracking allocated memory that is never deallocated.

3. **Memory Overhead:** Enabling KASAN introduces memory overhead due to the additional shadow memory and metadata used for tracking. This overhead varies depending on the configuration and the size of the kernel.

4. **Instrumentation Levels:** KASAN provides different instrumentation levels, such as `CONFIG_KASAN_GENERIC` and `CONFIG_KASAN_SW_TAGS`, allowing developers to choose the level of memory error detection and performance impact.

5. **Runtime Performance Impact:** While KASAN is invaluable for debugging and improving kernel security, it can impact runtime performance. The level of impact depends on the chosen configuration and hardware.

6. **Runtime Checks:** KASAN performs runtime checks during kernel execution, which means that it can catch memory errors as they occur, providing valuable diagnostic information.

7. **Integration with Kernel Build System:** KASAN can be enabled during the kernel build process by selecting the appropriate configuration options. This allows developers to build a kernel with KASAN support.

Here's an example of how to enable KASAN in the kernel configuration:

```bash
make menuconfig
# In the kernel configuration menu, navigate to "Kernel Hacking" ->
# "Compile-time checks and compiler options" and enable "Kernel Address
# Sanitizer (KASAN)" or its variations.
```

KASAN is a powerful tool for identifying memory errors early in the development process, improving the overall security and stability of the Linux kernel. It is particularly useful for finding security vulnerabilities and ensuring that the kernel's memory management is robust.


##  KFENCE


KFENCE, which stands for Kernel Electric Fence, is a dynamic memory safety tool for the Linux kernel. It is designed to detect and report various types of memory-related bugs in kernel code, similar to tools like Kernel Address Sanitizer (KASAN) and AddressSanitizer (ASan) for user-space applications. KFENCE focuses on detecting out-of-bounds accesses, use-after-free errors, and other memory safety issues in the kernel.

Key features and aspects of KFENCE:

1. **Memory Error Detection:** KFENCE instruments the kernel memory allocator to track memory allocations and deallocations. It adds padding around allocated memory regions and watches for any violations of these boundaries.

2. **Types of Bugs Detected:**
   - **Out-of-Bounds Access:** KFENCE detects reads or writes beyond the boundaries of allocated memory regions.
   - **Use-After-Free:** It identifies attempts to access memory after it has been freed, which helps prevent use-after-free vulnerabilities.
   - **Memory Leak Detection:** KFENCE can also track memory allocations and identify memory leaks by detecting allocated memory that is never freed.

3. **Low Memory Overhead:** KFENCE is designed to have a lower memory overhead compared to some other memory sanitization tools like KASAN. This makes it more suitable for production kernels where minimizing performance impact is crucial.

4. **Selective Instrumentation:** KFENCE allows for selective instrumentation of specific kernel modules or parts of the code, which can help minimize overhead in critical areas.

5. **Integration with Kernel Build System:** KFENCE can be enabled during the kernel build process by selecting the appropriate configuration options. This allows developers to build a kernel with KFENCE support.

Here's an example of how to enable KFENCE in the kernel configuration:

```bash
make menuconfig
# In the kernel configuration menu, navigate to "Kernel hacking" ->
# "Memory Debugging" and enable "Kernel Electric Fence (KFENCE)".
```

KFENCE provides valuable diagnostic information and helps kernel developers identify and fix memory safety issues early in the development process. It is a valuable addition to the set of tools available for improving the security and stability of the Linux kernel.
