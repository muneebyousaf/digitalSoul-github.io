---
title: 12. I/O Memory 
---
# Requesting I/O memory

 `request_mem_region` and `release_mem_region` are voluntary mechanisms provided by the Linux kernel to help prevent resource conflicts among drivers. Here's a bit more detail on their usage:

- **`request_mem_region`**:
  - This function is used by a driver to request exclusive access to a specified I/O memory region. It informs the kernel that the driver intends to use the memory addresses within the specified range.
  - Parameters:
    - `start`: The starting physical address of the memory region.
    - `len`: The length (size) of the memory region.
    - `name`: A string identifier for the driver or resource being reserved.
  - If the memory region is available and not already reserved by another driver, `request_mem_region` returns a pointer to a `struct resource` representing the reserved region. Otherwise, it returns `NULL`, indicating a failure.

- **`release_mem_region`**:
  - This function is used by a driver to release a previously reserved memory region when it is no longer needed. It informs the kernel that the driver is relinquishing its exclusive access to the specified memory region.
  - Parameters:
    - `start`: The starting physical address of the memory region.
    - `len`: The length (size) of the memory region.
  - Once a memory region is released using `release_mem_region`, it becomes available for other drivers to request and use.

These functions provide a way for drivers to cooperate and avoid conflicts when accessing I/O memory regions, but it's important to note that their usage depends on driver developers following best practices. Not all drivers may use these functions correctly, so it's still possible for conflicts to occur if drivers don't adhere to these voluntary mechanisms.

It's a good practice for driver developers to use `request_mem_region` when necessary and release memory regions appropriately to ensure a well-behaved and cooperative driver ecosystem in the Linux kernel.

# /proc/iomem 

The `/proc/iomem` file in a Linux system provides information about the memory regions that are currently reserved for I/O devices. It displays a list of memory ranges and their associated names, which helps users and developers understand how memory is allocated for hardware devices. This information is particularly useful for diagnosing resource conflicts and understanding the memory layout of a system.

Here's a general format of the `/proc/iomem` file:

```
start_address - end_address : resource_type
```

- `start_address` and `end_address` represent the range of physical memory addresses covered by the resource.
- `resource_type` is the name or identifier of the resource, typically associated with a device or driver.

Here's an example of what you might find in the `/proc/iomem` file:

```
00000000-00000fff : reserved
00001000-0009fbff : System RAM
0009fc00-0009ffff : reserved
000a0000-000bffff : Video RAM area
000c0000-000c7fff : Video ROM
000e0000-000fffff : reserved
00100000-7fffffff : System RAM
  00100000-002ac721 : Kernel code
  002ac722-003d0d23 : Kernel data
  00400000-005a5f13 : Kernel bss
80000000-801fffff : PCI Bus 0000:00
  80000000-801fffff : PCI Bus 0000:01
    80000000-8000ffff : 0000:01:00.0
```

In this example:

- The ranges `00001000-0009fbff` and `00100000-7fffffff` are designated as system RAM, indicating areas of physical memory used for general-purpose system memory.
- `000a0000-000bffff` represents the Video RAM area, which is commonly associated with graphics memory.
- `80000000-801fffff` is a PCI Bus region, and it's further divided into subregions for specific devices.

The `/proc/iomem` file is a valuable resource for understanding how physical memory is allocated across various devices in the system. It can help identify resource conflicts, understand memory layout, and troubleshoot issues related to hardware devices and memory access.

Keep in mind that the actual contents of `/proc/iomem` can vary depending on the specific hardware and drivers in use on your system.

 
# Mapping I/O memory in virtual memory

Mapping I/O memory into virtual memory is a common operation in Linux kernel programming, especially when working with hardware devices. This process allows device drivers and the kernel to access I/O registers and memory-mapped hardware registers in a way that's consistent with the rest of the virtual address space.

Here's a simplified overview of how I/O memory is mapped into virtual memory:

1. **Requesting I/O Memory Region**: First, the device driver typically requests access to the I/O memory region using the `request_mem_region` function. This function reserves a specific range of physical memory addresses for the driver's use. It helps prevent resource conflicts with other drivers.

   ```c
   struct resource *ioport;
   ioport = request_mem_region(start_address, length, "driver_name");
   if (ioport == NULL) {
       // Handle failure to request I/O memory region
   }
   ```

   `start_address` and `length` define the physical address range that the driver wants to access.

2. **Mapping I/O Memory**: To map the reserved I/O memory region into the kernel's virtual address space, the driver can use the `ioremap` function. This function returns a pointer to the mapped virtual memory space.

   ```c
   void __iomem *io_base;
   io_base = ioremap(start_address, length);
   if (io_base == NULL) {
       // Handle failure to map I/O memory
   }
   ```

   The `__iomem` annotation indicates that the pointer is used for memory-mapped I/O and helps the kernel maintain proper memory access semantics.

3. **Accessing I/O Memory**: Once the I/O memory region is mapped, the driver can access it just like regular memory by reading from and writing to the `io_base` pointer.

   ```c
   u32 value = readl(io_base);
   writel(new_value, io_base);
   ```

   The `readl` and `writel` functions are used to perform byte-order and synchronization operations to access the I/O registers correctly.

4. **Unmapping and Releasing Resources**: When the driver is done using the I/O memory region, it should unmap it from the virtual memory space and release the reserved resources.

   ```c
   iounmap(io_base);
   release_mem_region(start_address, length);
   ```

By following these steps, the driver can safely access and manipulate I/O memory as if it were regular memory in the virtual address space. This abstraction simplifies hardware interaction while maintaining proper memory protection and access control. It's important to note that I/O memory mapping is specific to kernel-level programming and should be used with caution to prevent hardware and system issues.


# Managed API

##  `devm_ioremap()` and `devm_iounmap()`

In the Linux kernel, `devm_ioremap()` and `devm_iounmap()` are memory mapping functions used to map I/O memory regions for a device driver. These functions are designed to be used with managed resources, which means that the memory mappings are automatically released when the device is removed or when the driver is unloaded. This helps prevent memory leaks and simplifies resource management in kernel code.

Here's an explanation of these functions:

1. **`devm_ioremap()`**:
   - This function is used to map an I/O memory region into the kernel's virtual address space.
   - It takes the following parameters:
     - `dev`: A pointer to the struct device associated with the device driver.
     - `phys_addr`: The physical address of the I/O memory region to be mapped.
     - `size`: The size (in bytes) of the memory region to be mapped.
   - It returns a pointer to the virtual address where the I/O memory region has been mapped.
   - The memory mapping obtained with `devm_ioremap()` is associated with the device and will be automatically unmapped when the device is removed or when the driver is unloaded.

2. **`devm_iounmap()`**:
   - This function is used to unmap an I/O memory region that was previously mapped with `devm_ioremap()`.
   - It takes the following parameters:
     - `dev`: A pointer to the struct device associated with the device driver.
     - `addr`: The virtual address of the I/O memory region to be unmapped.
   - It does not return a value.
   - When called, `devm_iounmap()` will unmap the specified I/O memory region, releasing the associated kernel resources.

Here's a simplified example of how these functions might be used in a device driver:

```c
#include <linux/ioport.h>
#include <linux/io.h>
#include <linux/module.h>
#include <linux/platform_device.h>

static int my_device_probe(struct platform_device *pdev)
{
    struct resource *iores;
    void __iomem *io_base;

    // Get the I/O memory resource from the platform device
    iores = platform_get_resource(pdev, IORESOURCE_MEM, 0);
    if (!iores) {
        dev_err(&pdev->dev, "IO resource not available\n");
        return -ENODEV;
    }

    // Map the I/O memory region using devm_ioremap()
    io_base = devm_ioremap(&pdev->dev, iores->start, resource_size(iores));
    if (!io_base) {
        dev_err(&pdev->dev, "Failed to map IO memory\n");
        return -ENOMEM;
    }

    // Access the hardware registers using io_base

    return 0;
}

static int my_device_remove(struct platform_device *pdev)
{
    // Unmap the I/O memory region using devm_iounmap()
    struct resource *iores = platform_get_resource(pdev, IORESOURCE_MEM, 0);
    void __iomem *io_base = dev_get_drvdata(&pdev->dev);
    if (iores && io_base)
        devm_iounmap(&pdev->dev, io_base);

    return 0;
}

// Other driver initialization and cleanup code

static struct platform_driver my_driver = {
    .probe = my_device_probe,
    .remove = my_device_remove,
    .driver = {
        .name = "my_device",
    },
};

module_platform_driver(my_driver);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
```

In this example, `devm_ioremap()` is used to map the I/O memory region in the `probe` function, and `devm_iounmap()` is used in the `remove` function to unmap it. The managed resource functions ensure that the memory mappings are automatically cleaned up when the driver is removed or when the associated device is unbound.

## `devm_ioremap_resource()`

`devm_ioremap_resource()` is a convenience function in the Linux kernel that simplifies the process of mapping I/O memory regions for device drivers. It is commonly used when working with platform devices. This function automatically handles resource allocation, mapping, and cleanup, making it easier to write robust and maintainable code.

Here's how `devm_ioremap_resource()` works:

1. It takes as input a pointer to the `struct device` (usually associated with a platform device) and an index to specify which I/O memory resource to map.

2. The function internally calls `platform_get_resource()` to retrieve the specified I/O memory resource.

3. It then uses `devm_ioremap()` to map the I/O memory region specified by the resource.

4. When the device is removed or when the driver is unloaded, `devm_ioremap_resource()` ensures that the mapped memory region is automatically unmapped, preventing memory leaks.

Here's an example of how to use `devm_ioremap_resource()` in a device driver:

```c
#include <linux/module.h>
#include <linux/platform_device.h>
#include <linux/ioport.h>

static int my_driver_probe(struct platform_device *pdev)
{
    struct resource *iores;
    void __iomem *io_base;

    // Get the I/O memory resource for this platform device
    iores = platform_get_resource(pdev, IORESOURCE_MEM, 0);
    if (!iores) {
        dev_err(&pdev->dev, "IO resource not available\n");
        return -ENODEV;
    }

    // Map the I/O memory region using devm_ioremap_resource()
    io_base = devm_ioremap_resource(&pdev->dev, iores);
    if (IS_ERR(io_base)) {
        dev_err(&pdev->dev, "Failed to map IO memory\n");
        return PTR_ERR(io_base);
    }

    // Access hardware registers using io_base

    return 0;
}

static int my_driver_remove(struct platform_device *pdev)
{
    // Nothing specific to unmap, as devm_ioremap_resource() handles cleanup
    return 0;
}

static struct platform_driver my_driver = {
    .probe = my_driver_probe,
    .remove = my_driver_remove,
    .driver = {
        .name = "my_device",
    },
};

module_platform_driver(my_driver);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Your Name");
```

In this example, `devm_ioremap_resource()` is used to map the I/O memory region specified by the first resource of type `IORESOURCE_MEM` for the platform device. The function takes care of resource allocation and mapping, and automatic cleanup is ensured when the driver is removed.
# Accessing MMIO devices: using accessor functions

Accessing Memory-Mapped I/O (MMIO) devices in the Linux kernel often involves using accessor functions to read and write data to and from the device registers. These accessor functions are designed to ensure proper synchronization, endianness handling, and other considerations specific to MMIO operations. Here's an overview of common accessor functions and their usage:

1. `readl()` and `writel()`:
   - These functions are used for 32-bit MMIO register access.
   - `readl(addr)` reads a 32-bit value from the MMIO address `addr`.
   - `writel(val, addr)` writes the 32-bit value `val` to the MMIO address `addr`.
   - Example usage:
     ```c
     u32 reg_value = readl(mmio_base + OFFSET);
     writel(new_value, mmio_base + OFFSET);
     ```

2. `readw()` and `writew()`:
   - These functions are used for 16-bit MMIO register access.
   - `readw(addr)` reads a 16-bit value from the MMIO address `addr`.
   - `writew(val, addr)` writes the 16-bit value `val` to the MMIO address `addr`.
   - Example usage:
     ```c
     u16 reg_value = readw(mmio_base + OFFSET);
     writew(new_value, mmio_base + OFFSET);
     ```

3. `readb()` and `writeb()`:
   - These functions are used for 8-bit MMIO register access.
   - `readb(addr)` reads an 8-bit value from the MMIO address `addr`.
   - `writeb(val, addr)` writes the 8-bit value `val` to the MMIO address `addr`.
   - Example usage:
     ```c
     u8 reg_value = readb(mmio_base + OFFSET);
     writeb(new_value, mmio_base + OFFSET);
     ```

4. `ioread32()` and `iowrite32()`:
   - These functions are similar to `readl()` and `writel()`, but they are suitable for use in various architectures and can handle byte-order differences automatically.
   - Example usage:
     ```c
     u32 reg_value = ioread32(mmio_base + OFFSET);
     iowrite32(new_value, mmio_base + OFFSET);
     ```

5. Memory Barriers:
   - When accessing MMIO registers in a multi-core or multi-processor environment, memory barriers may be necessary to ensure proper synchronization. The `readl_relaxed()`, `writel_relaxed()`, `ioread32_rep()`, and `iowrite32_rep()` functions are available for specific use cases.

The choice of accessor function depends on the data width of the MMIO registers and the specific requirements of your device. Always refer to the architecture-specific documentation and header files for the appropriate accessor functions to use on your target platform.

Additionally, make sure to use proper memory barriers and synchronization mechanisms when accessing MMIO registers to ensure that reads and writes are visible to the device and other cores or processors as intended.


## Memory barries 

Memory barriers are synchronization mechanisms used in concurrent programming to ensure proper memory ordering and consistency between threads or processors. In the context of memory-mapped I/O (MMIO) access functions like `ioreadbe` and `iowritebe` in the Linux kernel, memory barriers are essential for maintaining the correct sequence of memory operations when reading from or writing to hardware registers.

Here's how memory barriers work and why they are important in MMIO access:

1. **Memory Ordering**: In modern computer architectures, memory operations can be executed out of order for performance optimization. This means that reads and writes to memory locations can occur in a different order than they appear in the program's source code.

2. **Cache Coherency**: Many processors use caches to improve memory access speed. These caches may temporarily store data before writing it back to main memory. Without proper synchronization, cached values can lead to incorrect results when accessing MMIO registers.

3. **Atomicity**: Some MMIO operations must be atomic, meaning they should not be interrupted or reordered by the CPU or other devices. Memory barriers help enforce atomicity.

Memory barriers provide a way to control memory access ordering and enforce synchronization points between CPU cores or threads. In the case of MMIO access functions for big-endian devices, memory barriers ensure that:

- Reads and writes to MMIO registers occur in the expected order.
- Reads from the device are not reordered or optimized away.
- Writes to the device are not reordered or combined with other writes.

By including memory barriers in the MMIO access functions, developers can write reliable and efficient code for interacting with hardware devices, especially in a multi-core or multi-processor environment.

Here's a simplified example of why memory barriers are important:

```c
// Incorrect code without memory barrier
write_to_device_register(value);
read_from_device_register();  // Without a memory barrier, this could be reordered

// Correct code with memory barrier
write_to_device_register(value);
memory_barrier();  // Ensures the correct ordering of reads and writes
read_from_device_register();  // Now the read occurs after the write
```

In the Linux kernel, memory barrier functions like `smp_rmb()`, `smp_wmb()`, and others are used to enforce memory ordering and synchronization, and they are often included in MMIO access functions to ensure correct device interaction. These functions vary based on the specific memory ordering requirements of the target architecture.
# /dev/mem

`/dev/mem` is a special device file in Unix-like operating systems, including Linux, that provides direct access to the physical memory (RAM) of the computer. It allows privileged users, typically with root access, to read from and write to physical memory addresses. `/dev/mem` provides a way to interact with and manipulate memory-mapped hardware registers, perform low-level memory operations, and access specific areas of physical memory directly.

Here's an overview of `/dev/mem`:

1. **Device File**: `/dev/mem` is represented as a device file in the `/dev` directory of the Linux filesystem.

2. **Character Device**: It is a character device, which means it is accessed as a stream of characters. You can read from and write to `/dev/mem` as if you were reading and writing to a regular file.

3. **Memory Access**: `/dev/mem` allows you to access both RAM and memory-mapped hardware registers. You can read and write data to physical memory locations, which can be useful for tasks such as inspecting memory contents, debugging, and interacting with hardware devices.

4. **Security Implications**: Access to `/dev/mem` is restricted due to the potential for security risks. It typically requires superuser (root) privileges to access, as it allows direct manipulation of memory. Unauthorized access can lead to system instability or security vulnerabilities.

5. **Physical Memory Mapping**: `/dev/mem` provides a flat view of physical memory, which includes system RAM and any memory-mapped hardware devices. It does not provide access to kernel space, which is protected.

Here's an example of how you might use `/dev/mem` to read and modify physical memory:

```bash
# Read 4 bytes from physical memory address 0x10000000
sudo dd if=/dev/mem of=output.bin bs=4 count=1 skip=268435456

# Write data to physical memory address 0x10000000
echo -n -e '\x12\x34\x56\x78' | sudo dd of=/dev/mem bs=4 seek=268435456 conv=notrunc
```

In the example above:
- We use the `dd` command to read and write data.
- `if=/dev/mem` specifies the input file as `/dev/mem`.
- `of=output.bin` specifies the output file for reading.
- `bs=4` sets the block size to 4 bytes.
- `count=1` reads or writes one block.
- `skip=268435456` and `seek=268435456` specify the physical memory address 0x10000000 (in decimal) as the starting point for reading and writing. This address can be calculated based on your system's memory map.

Please note that accessing physical memory through `/dev/mem` can be risky, and incorrect operations can crash or damage your system. It is typically reserved for advanced debugging, hardware development, or specialized tasks. Regular user-space programs should not need to access physical memory directly.
